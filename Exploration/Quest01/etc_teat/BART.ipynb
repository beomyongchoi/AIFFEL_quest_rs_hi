{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6724ce-b7a9-424a-b228-ad03edc69970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from evaluate) (2.2.6)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.12/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from evaluate) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.12/site-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.5.1)\n",
      "Collecting huggingface-hub>=0.7.0 (from evaluate)\n",
      "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from evaluate) (25.0)\n",
      "Collecting nltk (from rouge_score)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.12/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Collecting dill (from evaluate)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading multidict-6.6.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.7.0->evaluate)\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk->rouge_score) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk->rouge_score) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk->rouge_score)\n",
      "  Downloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.6.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "Downloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "\u001b[33m  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=dbddb467cafc31fb3510209599282f25da64b3a90bd2179cb447dd71c37fbec4\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: xxhash, regex, propcache, multidict, hf-xet, fsspec, frozenlist, dill, aiohappyeyeballs, absl-py, yarl, nltk, multiprocess, huggingface-hub, aiosignal, rouge_score, aiohttp, datasets, evaluate\n",
      "\u001b[2K  Attempting uninstall: fsspec━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/19\u001b[0m [regex]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/19\u001b[0m [regex]\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.1:[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/19\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.1━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/19\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: dill\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/19\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: dill 0.4.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/19\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling dill-0.4.0:91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/19\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.0━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/19\u001b[0m [dill]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/19\u001b[0m [evaluate]/19\u001b[0m [datasets]ce-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.0.0 dill-0.3.8 evaluate-0.4.5 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.7 huggingface-hub-0.34.3 multidict-6.6.3 multiprocess-0.70.16 nltk-3.9.1 propcache-0.3.2 regex-2025.7.34 rouge_score-0.1.2 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score absl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0010e24-805b-4327-849b-8488b3a56d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6cbb1ab9094f7eb890075a3f0921b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a10766fa10140279bb585d591d82ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-ebc48879f34571(…):   0%|          | 0.00/1.54M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb04a363c1f40ddbc352bd5f76d93d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001-6227bd8eb10a9b5(…):   0%|          | 0.00/31.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c9386cd59c40669628473b9590bc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4142ada023164a84a6a6c399920dfce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/20417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source News : DANANG, Vietnam (Reuters) - Russian President Vladimir Putin said on Saturday he had a normal dialogue with U.S. leader Donald Trump at a summit in Vietnam, and described Trump as civil, well-educated\n",
      "Summarization : Putin says had useful interaction with Trump at Vi\n",
      "Training Data Size : 3000\n",
      "Validation Data Size : 1000\n",
      "Testing Data Size : 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Argilla 뉴스 요약 데이터셋 로드, 전처리 및 데이터 분할\n",
    "import numpy as np # 넘파이 라이브러리 임포트\n",
    "from datasets import load_dataset # Hugging Face의 datasets 라이브러리에서 load_dataset 함수 임포트\n",
    "\n",
    "# 'argilla/news-summary' 데이터셋을 불러와 'test' 분할을 선택합니다.\n",
    "# news = load_dataset(...) : 데이터셋을 news 변수에 할당합니다.\n",
    "news = load_dataset(\"argilla/news-summary\", split=\"test\")\n",
    "\n",
    "# 데이터셋을 판다스 데이터프레임으로 변환하고, 5000개 샘플을 무작위로 추출합니다.\n",
    "# df = news.to_pandas().sample(...) : 추출된 샘플을 df 변수에 할당합니다.\n",
    "df = news.to_pandas().sample(5000, random_state=42)[[\"text\", \"prediction\"]]\n",
    "\n",
    "# 'prediction' 컬럼의 데이터를 첫 번째 텍스트 값으로 매핑합니다.\n",
    "# .map(lambda x: x[0][\"text\"]) : 각 prediction 항목이 리스트 내 딕셔너리 형태이므로, 첫 번째 딕셔너리의 'text' 값을 추출합니다.\n",
    "df[\"prediction\"] = df[\"prediction\"].map(lambda x: x[0][\"text\"])\n",
    "\n",
    "# 전체 데이터를 6:2:2 비율로 훈련, 검증, 테스트 데이터셋으로 분할합니다.\n",
    "train, valid, test = np.split(\n",
    "    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))])\n",
    "\n",
    "# 첫 번째 훈련 데이터의 원문(text)과 요약문(prediction)의 일부를 출력합니다.\n",
    "print(f\"Source News : {train.text.iloc[0][:200]}\")\n",
    "print(f\"Summarization : {train.prediction.iloc[0][:50]}\")\n",
    "\n",
    "# 분할된 각 데이터셋의 크기를 출력합니다.\n",
    "print(f\"Training Data Size : {len(train)}\")\n",
    "print(f\"Validation Data Size : {len(valid)}\")\n",
    "print(f\"Testing Data Size : {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c738c5f-be7a-4821-8209-a7bd7492580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Downloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Installing collected packages: safetensors, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed safetensors-0.6.1 tokenizers-0.21.4 transformers-4.55.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903c347a-93c7-4d26-9fef-66b99edd3756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a41fbca29f54428a545df5511b4a6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2986615cd5dd4ac98b3d7cbf75a5bc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d47d43d3d1c473987d8eabea97d67fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8529e1b064449291fb0bc7ad64f1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([   0,  495, 1889,  ...,    1,    1,    1], device='cuda:0'), tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0'), tensor([    0, 35891,   161,    56,  5616, 10405,    19,   140,    23,  5490,\n",
      "         3564,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# Bart 모델 학습을 위한 데이터 전처리 및 데이터로더 생성\n",
    "import torch # 파이토치 라이브러리 임포트\n",
    "from transformers import BartTokenizer # Hugging Face의 transformers 라이브러리에서 BartTokenizer 임포트\n",
    "from torch.utils.data import TensorDataset, DataLoader # 파이토치 데이터 유틸리티 임포트\n",
    "from torch.utils.data import RandomSampler, SequentialSampler # 데이터 샘플러 임포트\n",
    "from torch.nn.utils.rnn import pad_sequence # 시퀀스 길이를 맞추기 위한 함수 임포트\n",
    "\n",
    "# 데이터셋을 만드는 함수를 정의합니다.\n",
    "def make_dataset(data, tokenizer, device):\n",
    "    # 입력 텍스트(원문)를 토크나이징합니다.\n",
    "    tokenized = tokenizer(\n",
    "        text=data.text.tolist(), # 데이터프레임의 'text' 컬럼을 리스트로 변환하여 입력으로 사용\n",
    "        padding=\"longest\", # 모든 시퀀스 길이를 가장 긴 시퀀스에 맞춰 패딩\n",
    "        truncation=True, # 시퀀스 길이가 최대 길이를 초과하면 잘라냄\n",
    "        return_tensors=\"pt\", # 파이토치 텐서 형태로 반환\n",
    "        max_length=1024 # 최대 시퀀스 길이를 1024로 설정\n",
    "    )\n",
    "    labels = [] # 라벨(요약문)을 저장할 빈 리스트 생성\n",
    "    input_ids = tokenized[\"input_ids\"].to(device) # 토큰 인덱스를 추출하여 device로 이동\n",
    "    attention_mask = tokenized[\"attention_mask\"].to(device) # 어텐션 마스크를 추출하여 device로 이동\n",
    "    \n",
    "    # 각 요약문(prediction)을 토크나이징하고 labels 리스트에 추가합니다.\n",
    "    for target in data.prediction:\n",
    "        labels.append(tokenizer.encode(target, return_tensors=\"pt\").squeeze())\n",
    "        # tokenizer.encode()는 텍스트를 토큰화하여 텐서로 반환합니다. .squeeze()로 차원을 축소합니다.\n",
    "    \n",
    "    # 여러 길이의 라벨 텐서를 가장 긴 길이에 맞춰 패딩합니다.\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100).to(device)\n",
    "    # padding_value=-100: 손실 계산 시 패딩 부분을 무시하도록 -100으로 설정합니다.\n",
    "    \n",
    "    # input_ids, attention_mask, labels를 묶어 TensorDataset 객체를 반환합니다.\n",
    "    return TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "# 데이터로더를 생성하는 함수입니다.\n",
    "def get_datalodader(dataset, sampler, batch_size):\n",
    "    data_sampler = sampler(dataset) # 데이터셋에 적용할 샘플러(Random 또는 Sequential)를 생성\n",
    "    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size) # DataLoader 생성\n",
    "    return dataloader\n",
    "\n",
    "# 하이퍼파라미터 및 디바이스 설정\n",
    "epochs = 5\n",
    "batch_size = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # GPU 사용 가능 여부 확인\n",
    "tokenizer = BartTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"facebook/bart-base\") # Bart 토크나이저 로드\n",
    "\n",
    "# make_dataset 함수를 사용하여 훈련, 검증, 테스트 데이터셋을 생성합니다.\n",
    "train_dataset = make_dataset(train, tokenizer, device)\n",
    "train_dataloader = get_datalodader(train_dataset, RandomSampler, batch_size)\n",
    "valid_dataset = make_dataset(valid, tokenizer, device)\n",
    "valid_dataloader = get_datalodader(valid_dataset, SequentialSampler, batch_size)\n",
    "test_dataset = make_dataset(test, tokenizer, device)\n",
    "test_dataloader = get_datalodader(test_dataset, SequentialSampler, batch_size)\n",
    "\n",
    "# 훈련 데이터셋의 첫 번째 요소를 출력하여 데이터 구조를 확인합니다.\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca844936-e914-405a-be13-c57e4741af3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08cfc804bdb44f2abac38675d6b4b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bart 모델 및 옵티마이저 설정\n",
    "from torch import optim # 파이토치에서 옵티마이저 모듈 임포트\n",
    "from transformers import BartForConditionalGeneration # Hugging Face의 transformers 라이브러리에서 BartForConditionalGeneration 클래스 임포트\n",
    "\n",
    "# BartForConditionalGeneration 모델을 불러옵니다.\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"facebook/bart-base\").to(device)\n",
    "# 'facebook/bart-base'는 사전 학습된 BART 모델의 이름입니다.\n",
    "# .to(device)는 모델을 GPU와 같은 가속기로 보내 연산을 빠르게 합니다.\n",
    "\n",
    "# AdamW 옵티마이저를 정의합니다.\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "# model.parameters(): 모델의 모든 학습 가능한 파라미터를 옵티마이저에 전달합니다.\n",
    "# lr=5e-5: 학습률(learning rate)을 0.00005로 설정합니다.\n",
    "# eps=1e-8: 부동 소수점 연산에서 0으로 나누는 것을 방지하기 위한 작은 상수(epsilon)를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "712dabbe-323e-4d29-be76-30f4b857564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "└ shared\n",
      "└ encoder\n",
      "│  └ embed_tokens\n",
      "│  └ embed_positions\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  └ layernorm_embedding\n",
      "└ decoder\n",
      "│  └ embed_tokens\n",
      "│  └ embed_positions\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  └ layernorm_embedding\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "# Bart 모델의 계층 구조 출력\n",
    "# model.named_children() : 모델의 직계 자식 모듈들을 (이름, 모듈) 쌍으로 반환합니다.\n",
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name) # 'model'과 'lm_head' 같은 최상위 모듈의 이름을 출력합니다.\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name) # 최상위 모듈의 하위 모듈 이름을 출력합니다. (예: 'model' 아래의 'shared', 'encoder', 'decoder')\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│  └\", ssub_name) # 하위 모듈의 하위 모듈 이름을 출력합니다. (예: 'encoder' 아래의 'embed_tokens', 'embed_positions', 'layers')\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                print(\"│  │  └\", sssub_name) # 가장 깊은 계층의 모듈 이름을 출력합니다. (예: 'layers' 아래의 '0'부터 '11'까지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bba2945-481e-4461-8280-6e744c736471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe22ad8ff32440b837482f2d0f6936f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.1637 Val Loss: 1.8464 Val Rouge 0.2535\n",
      "Saved the model weights\n",
      "Epoch 2: Train Loss: 1.6029 Val Loss: 1.8931 Val Rouge 0.2596\n",
      "Epoch 3: Train Loss: 1.2412 Val Loss: 2.0054 Val Rouge 0.2500\n",
      "Epoch 4: Train Loss: 0.9593 Val Loss: 2.1244 Val Rouge 0.2415\n",
      "Epoch 5: Train Loss: 0.8476 Val Loss: 2.2706 Val Rouge 0.2347\n"
     ]
    }
   ],
   "source": [
    "# Bart 모델 훈련, 검증 및 가중치 저장\n",
    "import numpy as np # 넘파이 라이브러리 임포트\n",
    "import evaluate # Hugging Face의 evaluate 라이브러리 임포트\n",
    "from tqdm.auto import tqdm # tqdm 라이브러리 임포트\n",
    "\n",
    "# ROUGE 점수를 계산하는 함수입니다.\n",
    "def calc_rouge(preds, labels):\n",
    "    # 예측값(preds)의 가장 높은 확률을 가진 클래스의 인덱스를 가져옵니다.\n",
    "    preds = preds.argmax(axis=-1)\n",
    "    # 라벨에서 패딩 값(-100)을 토크나이저의 패딩 토큰 ID로 대체합니다.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # 예측 토큰 ID를 텍스트로 디코딩합니다. 특수 토큰은 제외합니다.\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # 라벨 토큰 ID를 텍스트로 디코딩합니다. 특수 토큰은 제외합니다.\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE-2 점수를 계산합니다.\n",
    "    rouge2 = rouge_score.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels\n",
    "    )\n",
    "    return rouge2[\"rouge2\"] # ROUGE-2 점수만 반환합니다.\n",
    "\n",
    "# 모델을 훈련시키는 함수입니다.\n",
    "def train(model, optimizer, dataloader):\n",
    "    model.train() # 모델을 훈련 모드로 설정합니다.\n",
    "    train_loss = 0.0 # 훈련 손실을 초기화합니다.\n",
    "    # 데이터로더로부터 배치 단위로 데이터를 가져옵니다.\n",
    "    for input_ids, attention_mask, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        # 모델에 데이터를 입력하여 출력(outputs)을 얻습니다.\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels # 라벨은 손실 계산에 사용됩니다.\n",
    "        )\n",
    "        loss = outputs.loss # 출력에서 손실 값을 추출합니다.\n",
    "        train_loss += loss.item() # 손실 값을 누적합니다.\n",
    "        \n",
    "        optimizer.zero_grad() # 옵티마이저의 기울기를 초기화합니다.\n",
    "        loss.backward() # 손실에 대한 역전파를 수행하여 기울기를 계산합니다.\n",
    "        optimizer.step() # 옵티마이저를 사용하여 모델 파라미터를 업데이트합니다.\n",
    "\n",
    "    train_loss = train_loss / len(dataloader) # 배치당 평균 손실을 계산합니다.\n",
    "    return train_loss # 평균 훈련 손실을 반환합니다.\n",
    "\n",
    "# 모델을 평가하는 함수입니다.\n",
    "def evaluation(model, dataloader):\n",
    "    with torch.no_grad(): # 기울기 계산을 비활성화합니다.\n",
    "        model.eval() # 모델을 평가 모드로 설정합니다.\n",
    "        val_loss, val_rouge = 0.0, 0.0 # 검증 손실과 ROUGE 점수를 초기화합니다.\n",
    "        # 데이터로더로부터 배치 단위로 데이터를 가져옵니다.\n",
    "        for input_ids, attention_mask, labels in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            logits = outputs.logits # 출력에서 로짓(logits) 값을 추출합니다.\n",
    "            loss = outputs.loss # 출력에서 손실 값을 추출합니다.\n",
    "\n",
    "            logits = logits.detach().cpu().numpy() # 로짓을 CPU로 이동 후 넘파이 배열로 변환합니다.\n",
    "            label_ids = labels.to(\"cpu\").numpy() # 라벨을 CPU로 이동 후 넘파이 배열로 변환합니다.\n",
    "            rouge = calc_rouge(logits, label_ids) # ROUGE 점수를 계산합니다.\n",
    "            \n",
    "            val_loss += loss.item() # 손실 값을 누적합니다.\n",
    "            val_rouge += rouge # ROUGE 점수를 누적합니다.\n",
    "\n",
    "    val_loss = val_loss / len(dataloader) # 배치당 평균 손실을 계산합니다.\n",
    "    val_rouge = val_rouge / len(dataloader) # 배치당 평균 ROUGE 점수를 계산합니다.\n",
    "    return val_loss, val_rouge # 평균 손실과 ROUGE 점수를 반환합니다.\n",
    "\n",
    "# ROUGE 스코어 객체를 로드하고 토크나이저를 전달합니다.\n",
    "rouge_score = evaluate.load(\"rouge\", tokenizer=tokenizer)\n",
    "best_loss = 10000 # 최저 손실을 저장하기 위한 변수를 큰 값으로 초기화합니다.\n",
    "\n",
    "for epoch in range(epochs): # 지정된 에포크 수만큼 반복합니다.\n",
    "    print(f\"--- Epoch {epoch + 1}/{epochs} ---\") # 에포크 진행 상황을 시각적으로 표시\n",
    "    train_loss = train(model, optimizer, train_dataloader) # 훈련 함수를 호출합니다.\n",
    "    val_loss, val_accuracy = evaluation(model, valid_dataloader) # 평가 함수를 호출합니다.\n",
    "    # 에포크별 훈련 손실, 검증 손실, ROUGE 점수를 출력합니다.\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Val Rouge {val_accuracy:.4f}\")\n",
    "    # 현재 검증 손실이 기존의 최저 손실보다 낮으면 모델을 저장합니다.\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        # 모델의 가중치를 저장합니다.\n",
    "        torch.save(model.state_dict(), \"../models/BartForConditionalGeneration.pt\")\n",
    "        print(\"Saved the model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60454147-4803-4be4-82af-50900577f838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 1.8073\n",
      "Test ROUGE-2 Score : 0.2640\n"
     ]
    }
   ],
   "source": [
    "# 저장된 모델 가중치를 불러와 테스트 데이터셋으로 성능 평가\n",
    "from transformers import BartForConditionalGeneration # BartForConditionalGeneration 클래스를 임포트합니다.\n",
    "\n",
    "# BartForConditionalGeneration 모델을 불러옵니다.\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"facebook/bart-base\").to(device)\n",
    "# 'facebook/bart-base' 모델을 로드하고, 모델을 지정된 device(CPU 또는 GPU)로 이동시킵니다.\n",
    "\n",
    "# 저장된 모델의 가중치(state_dict)를 불러와 현재 모델에 적용합니다.\n",
    "# torch.load() 함수를 이용해 파일 경로에 있는 가중치 파일을 불러옵니다.\n",
    "# model.load_state_dict() 메서드를 사용하여 불러온 가중치를 모델에 로드합니다.\n",
    "model.load_state_dict(torch.load(\"../models/BartForConditionalGeneration.pt\"))\n",
    "\n",
    "# evaluation 함수를 사용하여 모델의 테스트 손실과 ROUGE-2 점수를 계산합니다.\n",
    "test_loss, test_rouge_score = evaluation(model, test_dataloader)\n",
    "\n",
    "# 계산된 테스트 손실과 테스트 ROUGE-2 점수를 출력합니다.\n",
    "print(f\"Test Loss : {test_loss:.4f}\")\n",
    "print(f\"Test ROUGE-2 Score : {test_rouge_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
