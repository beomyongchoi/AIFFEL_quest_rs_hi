{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6826465-e7a4-4304-9807-60db6a1bd17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Korpora\n",
      "  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting dataclasses>=0.6 (from Korpora)\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.12/site-packages (from Korpora) (2.2.6)\n",
      "Requirement already satisfied: tqdm>=4.46.0 in /opt/conda/lib/python3.12/site-packages (from Korpora) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.12/site-packages (from Korpora) (2.32.4)\n",
      "Requirement already satisfied: xlrd>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from Korpora) (2.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.20.0->Korpora) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.20.0->Korpora) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.20.0->Korpora) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.20.0->Korpora) (2025.6.15)\n",
      "Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: dataclasses, Korpora\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [Korpora]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Korpora-0.2.0 dataclasses-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdefcbe6-8f11-4656-93b1-f6479042694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e543d350-16d6-4364-b4d9-63bf162526ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/jovyan/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/jovyan/Korpora/nsmc/ratings_test.txt\n",
      "|       | text                                                     |   label |\n",
      "|------:|:---------------------------------------------------------|--------:|\n",
      "| 26891 | 역시 코믹액션은 성룡, 홍금보, 원표 삼인방이 최고지!!     |       1 |\n",
      "| 25024 | 점수 후하게 줘야것네 별 반개~                            |       0 |\n",
      "| 11666 | 오랜만에 느낄수 있는 [감독] 구타욕구.                    |       0 |\n",
      "| 40303 | 본지는 좀 됬지만 극장서 돈주고 본게 아직까지 아까운 영화 |       0 |\n",
      "| 18010 | 징키스칸이란 소재를 가지고 이것밖에 못만드냐             |       0 |\n",
      "Training Data Size : 12000\n",
      "Validation Data Size : 4000\n",
      "Testing Data Size : 4000\n"
     ]
    }
   ],
   "source": [
    "# 파일 불러오기 및 데이터 준비\n",
    "import numpy as np # 넘파이(Numpy) 라이브러리 불러오기\n",
    "import pandas as pd # 판다스(Pandas) 라이브러리 불러오기\n",
    "from Korpora import Korpora # Korpora 라이브러리에서 Korpora 클래스 불러오기\n",
    "\n",
    "# NSMC 데이터셋 로드 및 전처리\n",
    "corpus = Korpora.load(\"nsmc\") # Korpora를 사용해 네이버 영화 리뷰(nsmc) 데이터셋을 불러와 corpus 변수에 저장\n",
    "df = pd.DataFrame(corpus.test).sample(20000, random_state=42) # 테스트 데이터셋을 판다스 데이터프레임으로 변환하고, 20000개를 샘플링하여 df에 저장\n",
    "train, valid, test = np.split( # 데이터프레임을 학습, 검증, 테스트 셋으로 분리\n",
    "    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))]) # 전체 데이터의 60%를 학습, 20%를 검증, 20%를 테스트 데이터로 분할\n",
    "print(train.head(5).to_markdown()) # 학습 데이터의 상위 5개를 마크다운 형식으로 출력\n",
    "print(f\"Training Data Size : {len(train)}\") # 학습 데이터의 크기 출력\n",
    "print(f\"Validation Data Size : {len(valid)}\") # 검증 데이터의 크기 출력\n",
    "print(f\"Testing Data Size : {len(test)}\") # 테스트 데이터의 크기 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e08b5b4e-9f52-450a-98d9-e916b2e7131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Downloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Installing collected packages: safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.7 huggingface-hub-0.34.3 regex-2025.7.34 safetensors-0.6.1 tokenizers-0.21.4 transformers-4.55.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b337916d-9d50-4802-87e8-bc6f0af37cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d122939b2d54e94a847c5bc104f19cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/61.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc29bcd698d84048858f8e3b40570b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7393735a2d44cbbf237963978ea329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/467 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([    2,  6511, 14347,  4087,  4665,  4112,  2924,  4806,    16,  3809,\n",
      "         4309,  4275,    16,  3201,  4376,  2891,  4139,  4212,  4007,  6557,\n",
      "         4200,     5,     5,     3,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0], device='cuda:0'), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0], device='cuda:0'), tensor(1, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# 모델에 필요한 데이터셋 생성 및 데이터 로더 준비\n",
    "import torch # 파이토치(Pytorch) 라이브러리 불러오기\n",
    "from transformers import ElectraTokenizer # Hugging Face Transformers에서 ElectraTokenizer 불러오기\n",
    "from torch.utils.data import TensorDataset, DataLoader # Pytorch에서 TensorDataset과 DataLoader 불러오기\n",
    "from torch.utils.data import RandomSampler, SequentialSampler # Pytorch에서 RandomSampler와 SequentialSampler 불러오기\n",
    "\n",
    "def make_dataset(data, tokenizer, device): # 데이터셋을 만드는 함수 정의\n",
    "    tokenized = tokenizer( # 텍스트를 토큰화하고 인코딩하는 함수\n",
    "        text=data.text.tolist(), # 데이터의 텍스트 컬럼을 리스트로 변환\n",
    "        padding=\"longest\", # 가장 긴 시퀀스 길이에 맞춰 패딩\n",
    "        truncation=True, # 모델의 최대 입력 길이에 맞춰 자르기\n",
    "        return_tensors=\"pt\" # PyTorch 텐서 형식으로 반환\n",
    "    )\n",
    "    input_ids = tokenized[\"input_ids\"].to(device) # 토큰 ID를 디바이스로 이동\n",
    "    attention_mask = tokenized[\"attention_mask\"].to(device) # 어텐션 마스크를 디바이스로 이동\n",
    "    labels = torch.tensor(data.label.values, dtype=torch.long).to(device) # 라벨을 텐서로 변환하여 디바이스로 이동\n",
    "    return TensorDataset(input_ids, attention_mask, labels) # TensorDataset 객체로 반환\n",
    "\n",
    "def get_datalodader(dataset, sampler, batch_size): # 데이터 로더를 만드는 함수 정의\n",
    "    data_sampler = sampler(dataset) # 샘플러 객체 생성\n",
    "    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size) # 데이터 로더 생성\n",
    "    return dataloader # 데이터 로더 반환\n",
    "\n",
    "epochs = 5 # 에폭(학습 반복 횟수)을 5로 설정\n",
    "batch_size = 32 # 배치 사이즈를 32로 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # GPU 사용 가능 여부에 따라 디바이스 설정\n",
    "tokenizer = ElectraTokenizer.from_pretrained( # 사전 학습된 Electra 토크나이저 불러오기\n",
    "    pretrained_model_name_or_path=\"monologg/koelectra-base-v3-discriminator\", # KoELECTRA 기본 버전 3 사용\n",
    "    do_lower_case=False,) # 소문자 변환하지 않음\n",
    "train_dataset = make_dataset(train, tokenizer, device) # 학습 데이터셋 생성\n",
    "train_dataloader = get_datalodader(train_dataset, RandomSampler, batch_size) # 학습 데이터 로더 생성 (랜덤 샘플러 사용)\n",
    "valid_dataset = make_dataset(valid, tokenizer, device) # 검증 데이터셋 생성\n",
    "valid_dataloader = get_datalodader(valid_dataset, SequentialSampler, batch_size) # 검증 데이터 로더 생성 (순차 샘플러 사용)\n",
    "test_dataset = make_dataset(test, tokenizer, device) # 테스트 데이터셋 생성\n",
    "test_dataloader = get_datalodader(test_dataset, SequentialSampler, batch_size) # 테스트 데이터 로더 생성 (순차 샘플러 사용)\n",
    "print(train_dataset[0]) # 학습 데이터셋의 첫 번째 항목 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf78b2b5-1157-4ff4-a6e9-5d7142c22ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd268d8ad9442fa9a6be77da3684ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/452M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electra\n",
      "└ embeddings\n",
      "│  └ word_embeddings\n",
      "│  └ position_embeddings\n",
      "│  └ token_type_embeddings\n",
      "│  └ LayerNorm\n",
      "│  └ dropout\n",
      "└ encoder\n",
      "│  └ layer\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  │  └ 6\n",
      "│  │  └ 7\n",
      "│  │  └ 8\n",
      "│  │  └ 9\n",
      "│  │  └ 10\n",
      "│  │  └ 11\n",
      "classifier\n",
      "└ dense\n",
      "└ activation\n",
      "└ dropout\n",
      "└ out_proj\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4986655a3c2047cbb4ee59bd129aea19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/452M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델, 옵티마이저 정의 및 모델 구조 출력\n",
    "from torch import optim # Pytorch에서 optim 모듈 불러오기\n",
    "from transformers import ElectraForSequenceClassification # Hugging Face Transformers에서 ElectraForSequenceClassification 불러오기\n",
    "model = ElectraForSequenceClassification.from_pretrained( # 사전 학습된 KoELECTRA 모델 불러오기\n",
    "    pretrained_model_name_or_path=\"monologg/koelectra-base-v3-discriminator\", # KoELECTRA 기본 버전 3 사용\n",
    "    num_labels=2).to(device) # 출력 라벨 수를 2개(긍정/부정)로 설정하고 디바이스로 이동\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8) # AdamW 옵티마이저 정의\n",
    "\n",
    "for main_name, main_module in model.named_children(): # 모델의 최상위 모듈을 반복\n",
    "    print(main_name) # 최상위 모듈 이름 출력\n",
    "    for sub_name, sub_module in main_module.named_children(): # 하위 모듈을 반복\n",
    "        print(\"└\", sub_name) # 하위 모듈 이름 출력\n",
    "        for ssub_name, ssub_module in sub_module.named_children(): # 그 아래 하위 모듈을 반복\n",
    "            print(\"│  └\", ssub_name) # ...\n",
    "            for sssub_name, sssub_module in ssub_module.named_children(): # ...\n",
    "                print(\"│  │  └\", sssub_name) # 모델의 상세 구조를 계층적으로 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c554d5e-7289-4865-a79b-051c90e60a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 및 학습/평가 함수 정의\n",
    "import numpy as np # 넘파이(Numpy) 라이브러리 불러오기\n",
    "from torch import nn # Pytorch에서 nn 모듈 불러오기\n",
    "from tqdm import tqdm # 진행 바(Progress Bar)를 위한 tqdm 라이브러리 불러오기\n",
    "\n",
    "def calc_accuracy(preds, labels): # 정확도를 계산하는 함수 정의\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten() # 예측값 배열에서 가장 큰 값의 인덱스를 찾아 평탄화\n",
    "    labels_flat = labels.flatten() # 실제 라벨 배열을 평탄화\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat) # 예측값과 실제 라벨이 일치하는 비율 계산\n",
    "\n",
    "def train(model, optimizer, dataloader): # 모델을 학습하는 함수 정의\n",
    "    model.train() # 모델을 학습 모드로 설정\n",
    "    train_loss = 0.0 # 학습 손실 초기화\n",
    "\n",
    "    for input_ids, attention_mask, labels in tqdm(dataloader): # 데이터 로더에서 배치 단위로 데이터 가져오기\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels) # 모델에 데이터 입력하여 출력값 계산\n",
    "\n",
    "        loss = outputs.loss # 손실 값 가져오기\n",
    "        train_loss += loss.item() # 손실 누적\n",
    "        \n",
    "        optimizer.zero_grad() # 옵티마이저의 기울기 초기화\n",
    "        loss.backward() # 역전파를 통해 기울기 계산\n",
    "        optimizer.step() # 옵티마이저를 사용하여 파라미터 업데이트\n",
    "\n",
    "    train_loss = train_loss / len(dataloader) # 평균 학습 손실 계산\n",
    "    return train_loss # 평균 학습 손실 반환\n",
    "\n",
    "def evaluation(model, dataloader): # 모델을 평가하는 함수 정의\n",
    "    with torch.no_grad(): # 기울기 계산을 비활성화\n",
    "        model.eval() # 모델을 평가 모드로 설정\n",
    "        criterion = nn.CrossEntropyLoss() # 손실 함수로 교차 엔트로피 손실 사용\n",
    "        val_loss, val_accuracy = 0.0, 0.0 # 손실과 정확도 초기화\n",
    "        \n",
    "        for input_ids, attention_mask, labels in tqdm(dataloader): # 데이터 로더에서 배치 단위로 데이터 가져오기\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels) # 모델에 데이터 입력하여 출력값 계산\n",
    "            logits = outputs.logits # 로짓(최종 예측값) 가져오기\n",
    "\n",
    "            loss = criterion(logits, labels) # 로짓과 실제 라벨을 사용하여 손실 계산\n",
    "            logits = logits.detach().cpu().numpy() # 로짓을 CPU로 이동하고 넘파이 배열로 변환\n",
    "            label_ids = labels.to(\"cpu\").numpy() # 라벨을 CPU로 이동하고 넘파이 배열로 변환\n",
    "            accuracy = calc_accuracy(logits, label_ids) # 정확도 계산\n",
    "            \n",
    "            val_loss += loss.item() # 손실 누적\n",
    "            val_accuracy += accuracy # 정확도 누적\n",
    "    \n",
    "    val_loss = val_loss/len(dataloader) # 평균 검증 손실 계산\n",
    "    val_accuracy = val_accuracy/len(dataloader) # 평균 검증 정확도 계산\n",
    "    return val_loss, val_accuracy # 평균 손실과 정확도 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49886f0c-d68e-4e48-b802-e3ed2024c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [04:07<00:00,  1.52it/s]\n",
      "100%|██████████| 125/125 [00:27<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.4397 Val Loss: 0.3188 Val Accuracy 0.8700\n",
      "Saved the model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [04:06<00:00,  1.52it/s]\n",
      "100%|██████████| 125/125 [00:27<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.2775 Val Loss: 0.2953 Val Accuracy 0.8810\n",
      "Saved the model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [04:06<00:00,  1.52it/s]\n",
      "100%|██████████| 125/125 [00:27<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.2115 Val Loss: 0.3390 Val Accuracy 0.8702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [04:06<00:00,  1.52it/s]\n",
      "100%|██████████| 125/125 [00:27<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.1551 Val Loss: 0.3492 Val Accuracy 0.8838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [04:06<00:00,  1.52it/s]\n",
      "100%|██████████| 125/125 [00:27<00:00,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.1075 Val Loss: 0.3980 Val Accuracy 0.8788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 학습 실행\n",
    "best_loss = 10000 # 최적 손실 초기값 설정\n",
    "for epoch in range(epochs): # 지정된 에폭 수만큼 반복\n",
    "    train_loss = train(model, optimizer, train_dataloader) # 모델 학습 함수 호출\n",
    "    val_loss, val_accuracy = evaluation(model, valid_dataloader) # 모델 평가 함수 호출\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Val Accuracy {val_accuracy:.4f}\") # 에폭별 결과 출력\n",
    "\n",
    "    if val_loss < best_loss: # 현재 검증 손실이 이전 최적 손실보다 작으면\n",
    "        best_loss = val_loss # 최적 손실 업데이트\n",
    "        torch.save(model.state_dict(), \"../models/ElectraForSequenceClassification.pt\") # 모델 가중치 저장\n",
    "        print(\"Saved the model weights\") # 저장 메시지 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9adcfe5-cf0d-49b2-846d-8d7346962b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 125/125 [00:33<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.3107\n",
      "Test Accuracy : 0.8712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 실행\n",
    "model = ElectraForSequenceClassification.from_pretrained( # 사전 학습된 KoELECTRA 모델 불러오기\n",
    "    pretrained_model_name_or_path=\"monologg/koelectra-base-v3-discriminator\", # KoELECTRA 기본 버전 3 사용\n",
    "    num_labels=2).to(device) # 출력 라벨 수를 2개로 설정하고 디바이스로 이동\n",
    "model.load_state_dict(torch.load(\"../models/ElectraForSequenceClassification.pt\")) # 저장된 모델 가중치 불러오기\n",
    "test_loss, test_accuracy = evaluation(model, test_dataloader) # 테스트 데이터셋으로 모델 평가\n",
    "print(f\"Test Loss : {test_loss:.4f}\") # 테스트 손실 출력\n",
    "print(f\"Test Accuracy : {test_accuracy:.4f}\") # 테스트 정확도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c0351-a08d-4525-9a12-f169db66e8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
