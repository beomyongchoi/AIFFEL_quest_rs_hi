{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aadb3655-25fb-44af-b57d-05b1bc604c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.34.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bcae9ed-2548-4bcd-acf5-3b58d1a8f76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source News : summarize: DANANG, Vietnam (Reuters) - Russian President Vladimir Putin said on Saturday he had a normal dialogue with U.S. leader Donald Trump at a summit in Vietnam, and described Trump as civil, we\n",
      "Summarization : Putin says had useful interaction with Trump at Vi\n",
      "Training Data Size : 3000\n",
      "Validation Data Size : 1000\n",
      "Testing Data Size : 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# T5를 이용한 뉴스 요약 모델 학습 및 평가\n",
    "import numpy as np # 과학 기술 계산을 위한 라이브러리인 NumPy를 불러옵니다.\n",
    "from datasets import load_dataset # Hugging Face `datasets` 라이브러리에서 데이터셋을 불러오는 함수를 불러옵니다.\n",
    "\n",
    "# argilla/news-summary 데이터셋을 불러와 'test' 분할을 사용합니다.\n",
    "news = load_dataset(\"argilla/news-summary\", split=\"test\")\n",
    "\n",
    "# 데이터셋을 pandas DataFrame으로 변환하고 5000개의 샘플을 랜덤하게 추출합니다.\n",
    "df = news.to_pandas().sample(5000, random_state=42)[[\"text\", \"prediction\"]]\n",
    "\n",
    "# 'summarize: ' 접두사를 'text' 열에 추가하여 모델이 요약 작업임을 알 수 있게 합니다.\n",
    "df[\"text\"] = \"summarize: \" + df[\"text\"]\n",
    "\n",
    "# 'prediction' 열의 값을 리스트에서 첫 번째 텍스트 요소로 추출합니다.\n",
    "df[\"prediction\"] = df[\"prediction\"].map(lambda x: x[0][\"text\"])\n",
    "\n",
    "# 전체 데이터셋을 60% (학습), 20% (검증), 20% (테스트)로 분할합니다.\n",
    "train, valid, test = np.split(\n",
    "    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))])\n",
    "\n",
    "# 분할된 데이터의 첫 번째 샘플과 데이터셋 크기를 출력합니다.\n",
    "print(f\"Source News : {train.text.iloc[0][:200]}\")\n",
    "print(f\"Summarization : {train.prediction.iloc[0][:50]}\")\n",
    "print(f\"Training Data Size : {len(train)}\")\n",
    "print(f\"Validation Data Size : {len(valid)}\")\n",
    "print(f\"Testing Data Size : {len(test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad7bed4-57ff-4661-812d-90e028ba8c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab58d011-95f8-400f-a940-fcc6d7a2b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[21603,    10,    41,  ...,     0,     0,     0],\n",
      "        [21603,    10,   549,  ...,  4828,  8286,     1],\n",
      "        [21603,    10,   549,  ...,    12,    80,     1],\n",
      "        ...,\n",
      "        [21603,    10,   445,  ...,     0,     0,     0],\n",
      "        [21603,    10,  6554,  ...,   471,     6,     1],\n",
      "        [21603,    10,    41,  ...,  2876,  2721,     1]], device='cuda:0'), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), tensor([[    3, 15038,   667,  ...,     0,     0,     0],\n",
      "        [22439, 22121,     7,  ...,     0,     0,     0],\n",
      "        [27463, 22121,     7,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [12605,    29,  2095,  ...,     0,     0,     0],\n",
      "        [  412,     5,   134,  ...,     0,     0,     0],\n",
      "        [11543,  2689,    10,  ...,     0,     0,     0]], device='cuda:0'), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')]\n",
      "▁summarize\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "# T5 모델 학습을 위한 데이터셋과 데이터로더를 준비합니다.\n",
    "import torch # 딥러닝 프레임워크인 PyTorch를 불러옵니다.\n",
    "from transformers import T5Tokenizer # T5 모델의 토크나이저를 불러옵니다.\n",
    "from torch.utils.data import TensorDataset, DataLoader # PyTorch의 데이터셋과 데이터로더 클래스를 불러옵니다.\n",
    "from torch.utils.data import RandomSampler, SequentialSampler # 데이터 샘플러를 불러옵니다.\n",
    "from torch.nn.utils.rnn import pad_sequence # 시퀀스 패딩 유틸리티를 불러옵니다.\n",
    "\n",
    "# DataFrame 데이터를 T5 모델이 처리할 수 있는 형식의 데이터셋으로 변환하는 함수를 정의합니다.\n",
    "def make_dataset(data, tokenizer, device):\n",
    "    # 입력 텍스트를 토큰화하고 패딩, 자르기를 수행합니다.\n",
    "    source = tokenizer(\n",
    "        text=data.text.tolist(),\n",
    "        padding=\"max_length\", # 최대 길이로 패딩합니다.\n",
    "        max_length=128, # 최대 시퀀스 길이를 128로 설정합니다.\n",
    "        #pad_to_max_length=True, # 최대 길이로 패딩을 강제합니다.\n",
    "        truncation=True, # 최대 길이를 초과하는 텍스트를 자릅니다.\n",
    "        return_tensors=\"pt\" # PyTorch 텐서 형식으로 반환합니다.\n",
    "    )\n",
    "\n",
    "    # 목표(요약) 텍스트를 토큰화하고 패딩, 자르기를 수행합니다.\n",
    "    target = tokenizer(\n",
    "        text=data.prediction.tolist(),\n",
    "        padding=\"max_length\", \n",
    "        max_length=128,\n",
    "        #pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 생성된 텐서들을 지정된 디바이스(CPU/GPU)로 옮깁니다.\n",
    "    source_ids = source[\"input_ids\"].squeeze().to(device)\n",
    "    source_mask = source[\"attention_mask\"].squeeze().to(device)\n",
    "    target_ids = target[\"input_ids\"].squeeze().to(device)\n",
    "    target_mask = target[\"attention_mask\"].squeeze().to(device)\n",
    "    # 텐서들로 구성된 TensorDataset 객체를 반환합니다.\n",
    "    return TensorDataset(source_ids, source_mask, target_ids, target_mask)\n",
    "\n",
    "# 데이터셋과 샘플러를 이용하여 데이터로더를 생성하는 함수를 정의합니다.\n",
    "def get_datalodader(dataset, sampler, batch_size):\n",
    "    # 주어진 샘플러를 사용하여 데이터를 샘플링합니다.\n",
    "    data_sampler = sampler(dataset)\n",
    "    # 데이터셋과 샘플러, 배치 크기를 사용하여 데이터로더를 생성합니다.\n",
    "    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "# 학습에 사용될 하이퍼파라미터 및 설정을 정의합니다.\n",
    "epochs = 5\n",
    "batch_size = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # GPU 사용 가능 여부에 따라 디바이스를 설정합니다.\n",
    "\n",
    "# 't5-small' 사전 학습 모델의 토크나이저를 불러옵니다.\n",
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"t5-small\")\n",
    "\n",
    "# 학습, 검증, 테스트 데이터셋을 생성합니다.\n",
    "train_dataset = make_dataset(train, tokenizer, device)\n",
    "# 학습 데이터로더를 생성합니다. (랜덤 샘플링)\n",
    "train_dataloader = get_datalodader(train_dataset, RandomSampler, batch_size)\n",
    "valid_dataset = make_dataset(valid, tokenizer, device)\n",
    "# 검증 데이터로더를 생성합니다. (순차적 샘플링)\n",
    "valid_dataloader = get_datalodader(valid_dataset, SequentialSampler, batch_size)\n",
    "test_dataset = make_dataset(test, tokenizer, device)\n",
    "# 테스트 데이터로더를 생성합니다. (순차적 샘플링)\n",
    "test_dataloader = get_datalodader(test_dataset, SequentialSampler, batch_size)\n",
    "\n",
    "# 학습 데이터로더에서 첫 번째 배치를 출력하여 데이터 형식을 확인합니다.\n",
    "print(next(iter(train_dataloader)))\n",
    "# 특정 토큰 ID를 실제 토큰으로 변환하여 확인합니다.\n",
    "print(tokenizer.convert_ids_to_tokens(21603))\n",
    "print(tokenizer.convert_ids_to_tokens(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27871189-753c-4d75-a444-ee8f10be0afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dd28bfce96466592fc0bdf1310b7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea6409fbb154339ba0d8304e18fb378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd111da14b514063874a97e378053f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# T5 모델과 옵티마이저를 설정합니다.\n",
    "from torch import optim # PyTorch의 최적화 함수를 불러옵니다.\n",
    "from transformers import T5ForConditionalGeneration # T5 모델 클래스를 불러옵니다.\n",
    "\n",
    "# 't5-small' 사전 학습 모델을 불러와 지정된 디바이스로 옮깁니다.\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"t5-small\",).to(device)\n",
    "# AdamW 옵티마이저를 사용하여 모델의 파라미터를 최적화하도록 설정합니다.\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2267626f-ad2f-4452-81e4-a7ebfceace11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd5821b80a44955a72406f810a4ed58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2278506179a84134ab118eb92d1f11ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 4.3381 Val Loss: 3.3380\n",
      "Saved the model weights\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0560e30bcb0437dbf1b3e745ca6b425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b168041f8324c9fbc97f6ad5116dd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 3.4315 Val Loss: 2.9249\n",
      "Saved the model weights\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0ee836961d4ef09f4877d4f902acd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e591034120854edcbc7752c3fec4d525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 3.1551 Val Loss: 2.7705\n",
      "Saved the model weights\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4bfb0073fb4b90a9307f8d705723f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11365d842d264c118e7634f2afcfa27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 2.9997 Val Loss: 2.6775\n",
      "Saved the model weights\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e6336fe8a34ee186706518e4fce537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448ea9e4844c47dba3df1b4efe44bb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 2.9026 Val Loss: 2.6135\n",
      "Saved the model weights\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 및 평가에 필요한 함수들을 정의합니다.\n",
    "import numpy as np # NumPy를 다시 불러옵니다.\n",
    "from torch import nn # PyTorch의 신경망 모듈을 불러옵니다.\n",
    "from tqdm.auto import tqdm # tqdm 라이브러리를 불러와 진행 상태 바를 표시합니다.\n",
    "\n",
    "# 예측 결과와 레이블을 비교하여 정확도를 계산하는 함수입니다.\n",
    "def calc_accuracy(preds, labels):\n",
    "    # 예측 텐서를 평탄화하고 가장 높은 값의 인덱스를 취합니다.\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    # 레이블 텐서를 평탄화합니다.\n",
    "    labels_flat = labels.flatten()\n",
    "    # 예측과 레이블이 일치하는 비율을 계산하여 반환합니다.\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# 모델 학습을 위한 함수입니다.\n",
    "def train(model, optimizer, dataloader):\n",
    "    model.train() # 모델을 학습 모드로 설정합니다.\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # 데이터로더로부터 배치를 반복하여 가져옵니다.\n",
    "    for source_ids, source_mask, target_ids, target_mask in tqdm(dataloader, desc=\"Training\"):\n",
    "        # 디코더 입력은 목표 텐서의 첫 번째 토큰을 제외한 부분입니다.\n",
    "        decoder_input_ids = target_ids[:, :-1].contiguous()\n",
    "        # 레이블은 목표 텐서의 첫 번째 토큰을 제외한 부분입니다.\n",
    "        labels = target_ids[:, 1:].clone().detach()\n",
    "        # 패딩 토큰 ID를 -100으로 변경하여 손실 계산에서 제외합니다.\n",
    "        labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        # 모델에 입력을 전달하고 출력을 계산합니다.\n",
    "        outputs = model(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=source_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss # 손실 값을 가져옵니다.\n",
    "        train_loss += loss.item() # 손실을 누적합니다.\n",
    "\n",
    "        optimizer.zero_grad() # 옵티마이저의 기울기를 초기화합니다.\n",
    "        loss.backward() # 손실에 대한 역전파를 수행합니다.\n",
    "        optimizer.step() # 옵티마이저를 업데이트하여 모델의 가중치를 수정합니다.\n",
    "\n",
    "    train_loss = train_loss / len(dataloader) # 평균 학습 손실을 계산합니다.\n",
    "    return train_loss\n",
    "\n",
    "# 모델 검증을 위한 함수입니다.\n",
    "def evaluation(model, dataloader):\n",
    "    with torch.no_grad(): # 기울기 계산을 비활성화하여 메모리를 절약하고 속도를 높입니다.\n",
    "        model.eval() # 모델을 평가 모드로 설정합니다.\n",
    "        val_loss = 0.0\n",
    "\n",
    "        # 데이터로더로부터 배치를 반복하여 가져옵니다.\n",
    "        for source_ids, source_mask, target_ids, target_mask in tqdm(dataloader, desc=\"Validation\"):\n",
    "            # 디코더 입력과 레이블을 학습 시와 동일하게 설정합니다.\n",
    "            decoder_input_ids = target_ids[:, :-1].contiguous()\n",
    "            labels = target_ids[:, 1:].clone().detach()\n",
    "            labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "            # 모델에 입력을 전달하고 출력을 계산합니다.\n",
    "            outputs = model(\n",
    "                input_ids=source_ids,\n",
    "                attention_mask=source_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(dataloader) # 평균 검증 손실을 계산합니다.\n",
    "    return val_loss\n",
    "\n",
    "# 모델 학습 및 검증 루프입니다.\n",
    "best_loss = 10000 # 가장 낮은 검증 손실을 저장하기 위한 변수를 초기화합니다.\n",
    "for epoch in range(epochs):\n",
    "    # 학습 함수를 호출하여 한 에포크 동안 학습을 수행합니다.\n",
    "    train_loss = train(model, optimizer, train_dataloader)\n",
    "    # 검증 함수를 호출하여 한 에포크 동안 검증을 수행합니다.\n",
    "    val_loss = evaluation(model, valid_dataloader)\n",
    "    # 현재 에포크의 학습 및 검증 손실을 출력합니다.\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # 현재 검증 손실이 이전까지의 최소 손실보다 작으면 모델을 저장합니다.\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"../models/T5ForConditionalGeneration.pt\")\n",
    "        print(\"Saved the model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78fa4e97-1ec4-402c-ab55-af30cf032b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Headline Text: Clinton leads Trump by 4 percentage points in four-war race for Nov. 8 election\n",
      "Actual Headline Text   : Clinton leads Trump by 4 points in Washington Post: ABC News poll\n",
      "Generated Headline Text: U.S. senators sharpen potential line of attack against Gorsuch's nomination to Supreme Court\n",
      "Actual Headline Text   : Democrats question independence of Trump Supreme Court nominee\n",
      "Generated Headline Text: U.S. warns Saudi Arabia over humanitarian situation in Yemen could constrain U.S. aid, a U.S. official says\n",
      "Actual Headline Text   : In push for Yemen aid, U.S. warned Saudis of threats in Congress\n",
      "Generated Headline Text: Romanian anti-corruption prosecutors open investigation into Liviu Dragnea on suspicion of forming criminal group to siphon off cash from state projects\n",
      "Actual Headline Text   : Romanian ruling party leader investigated over 'criminal group'\n",
      "Generated Headline Text: environmental activist endorsed Hillary Clinton for U.S. president\n",
      "Actual Headline Text   : Billionaire environmental activist Tom Steyer endorses Clinton\n",
      "Generated Headline Text: tv presenter delivers news of Pyongyang nuclear test with her usual gusto.\n",
      "Actual Headline Text   : Voice of triumph or doom: North Korean presenter back in limelight for nuclear test\n",
      "Generated Headline Text: Delson Guarate and Yon Goicoechea among nearly 400 jailed anti-Maduro activists.\n",
      "Actual Headline Text   : Venezuela frees two anti-Maduro activists; scores still jailed\n",
      "Generated Headline Text: House Majority Leader says he still troubled by Clinton email server\n",
      "Actual Headline Text   : House No. 2 Republican says still questions Clinton's judgment in email matter\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터에 대한 요약 생성을 시작합니다.\n",
    "model.eval() # 모델을 평가 모드로 설정합니다.\n",
    "with torch.no_grad(): # 기울기 계산을 비활성화합니다.\n",
    "    # 테스트 데이터로더에서 배치를 가져옵니다.\n",
    "    for source_ids, source_mask, target_ids, target_mask in test_dataloader:\n",
    "        # 모델의 `generate` 함수를 사용하여 요약 텍스트를 생성합니다.\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=source_mask,\n",
    "            max_length=128, # 생성될 시퀀스의 최대 길이를 설정합니다.\n",
    "            num_beams=3, # 빔 서치(beam search)의 크기를 3으로 설정합니다.\n",
    "            repetition_penalty=2.5, # 반복 패널티를 설정합니다.\n",
    "            length_penalty=1.0, # 길이 패널티를 설정합니다.\n",
    "            early_stopping=True, # 조기 종료를 활성화합니다.\n",
    "        )\n",
    "\n",
    "        # 생성된 요약과 실제 요약을 비교하고 출력합니다.\n",
    "        for generated, target in zip(generated_ids, target_ids):\n",
    "            # 생성된 토큰 ID를 텍스트로 디코딩합니다.\n",
    "            pred = tokenizer.decode(\n",
    "                generated, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            # 실제 토큰 ID를 텍스트로 디코딩합니다.\n",
    "            actual = tokenizer.decode(\n",
    "                target, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            print(\"Generated Headline Text:\", pred) \n",
    "            print(\"Actual Headline Text   :\", actual) \n",
    "        break # 첫 번째 배치만 처리하고 루프를 종료합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211d0e5-6674-46a6-83d6-4071d9b7de4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
