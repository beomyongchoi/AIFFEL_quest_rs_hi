{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "839e92df-adb0-4ff3-ac2b-c7853002f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#네이버 영화 리뷰 감성 분석\n",
    "#6가지 실험 \n",
    "# 1_1 LSTM 모델 random 임배딩 사용\n",
    "# 1_2 LSTM 모델 word2vec 임배딩 사용\n",
    "# 2_1 cnn 모델 random 임배딩 사용\n",
    "# 2_2 cnn 모델 word2vec 임배딩 사용\n",
    "# 3_1 LSTM+cnn 모델 random 임배딩 사용\n",
    "# 3_2 LSTM+cnn 모델 word2vec 임배딩 사용\n",
    "\n",
    "#6가지의 임배딩 파일 비교 분석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "110428b5-ecc0-4fa5-861d-07bed7a59309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
    "#!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
    "#!mv ratings_*.txt ~/work/workplace/AIFFEL_quest_rs/Exploration/Quest05/sentiment_classification/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d3ffb4-408d-4f78-9139-29a1ec41811c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /opt/conda/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.12/site-packages (from konlpy) (1.26.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
      "remote: Enumerating objects: 138, done.\u001b[K\n",
      "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
      "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
      "remote: Total 138 (delta 26), reused 22 (delta 8), pack-reused 91 (from 1)\u001b[K\n",
      "Receiving objects: 100% (138/138), 1.72 MiB | 6.16 MiB/s, done.\n",
      "Resolving deltas: 100% (65/65), done.\n",
      "/home/jovyan/work/workplace/AIFFEL_quest_rs/Exploration/Quest05/Mecab-ko-for-Google-Colab\n",
      "install_mecab-ko_on_colab_light_220429.sh: line 4: cd: /content: No such file or directory\n",
      "Installing konlpy.....\n",
      "Requirement already satisfied: konlpy in /opt/conda/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.12/site-packages (from konlpy) (1.26.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Done\n",
      "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
      "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
      "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
      "--2025-08-05 05:34:39--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
      "Resolving bitbucket.org (bitbucket.org)... 104.192.143.22, 104.192.143.21, 104.192.143.23, ...\n",
      "Connecting to bitbucket.org (bitbucket.org)|104.192.143.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNADYRPKID&Signature=ljivSoZBUg1t5m70wA0W2J1kVlI%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEB4aCXVzLWVhc3QtMSJHMEUCIA8FdqRGN7%2Fp7wGcjKVABhLgCJqHM9kIwdrPU7g3416TAiEAyCojKrSM%2F8bFzOgf9waKPrEuH8%2B0PmeR67uwnOFUip8qpwIIVxAAGgw5ODQ1MjUxMDExNDYiDN8vGEL9SXqfsmiSDyqEAsHHSEruPXEFw5KyzT9ovaFwkdnXmicgAUB8IKzaoZnzpRpK5HAks066THEFfIFRnmGzNtRmKfkM7a%2B2CVG2XEgh4l6bp1DYNFLGGkf8F5UHdOBzm4Jv2aOwYulrIxM2IpJjQcavMJyB6Cn%2FALHRBRGzwYnnfi70vJ7mnyaz1hixHDxePikUnzkRN%2BosC5I0Z0HM0%2F8AXQPVsPXyfggloGkHukqzb6u9LedkSWyhwdBkYh669XOYWflu6cpVK08yRAmEDU4oowd6GYyoYGT2yyvGJJF91dl7Nfq2fXxQKHJyGwNw%2FfYT33oI3qV%2FzKtmuDMoU0YV6HhfoydFZM1oKoIrN95sMPCvxsQGOp0BcxTpxsZRRQiwdSFJdYsjbisbP%2FXnzYQDcutT54eaOfOpDClPGyPmcUexns9Eja1s5A2I8Cg4IdyX%2B%2BDkVdOKr%2BB5pjxITAaeLzMYan46wLGCXQFwV4lArKUziSFw7RuCHjE6houiC5%2FdmG%2FEQTHgr5de5aqyYvaP2rg69heR%2BRX6kvlXls9XFgInGU9mHjOyrgde2odQA1fpl422kA%3D%3D&Expires=1754373880 [following]\n",
      "--2025-08-05 05:34:41--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNADYRPKID&Signature=ljivSoZBUg1t5m70wA0W2J1kVlI%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEB4aCXVzLWVhc3QtMSJHMEUCIA8FdqRGN7%2Fp7wGcjKVABhLgCJqHM9kIwdrPU7g3416TAiEAyCojKrSM%2F8bFzOgf9waKPrEuH8%2B0PmeR67uwnOFUip8qpwIIVxAAGgw5ODQ1MjUxMDExNDYiDN8vGEL9SXqfsmiSDyqEAsHHSEruPXEFw5KyzT9ovaFwkdnXmicgAUB8IKzaoZnzpRpK5HAks066THEFfIFRnmGzNtRmKfkM7a%2B2CVG2XEgh4l6bp1DYNFLGGkf8F5UHdOBzm4Jv2aOwYulrIxM2IpJjQcavMJyB6Cn%2FALHRBRGzwYnnfi70vJ7mnyaz1hixHDxePikUnzkRN%2BosC5I0Z0HM0%2F8AXQPVsPXyfggloGkHukqzb6u9LedkSWyhwdBkYh669XOYWflu6cpVK08yRAmEDU4oowd6GYyoYGT2yyvGJJF91dl7Nfq2fXxQKHJyGwNw%2FfYT33oI3qV%2FzKtmuDMoU0YV6HhfoydFZM1oKoIrN95sMPCvxsQGOp0BcxTpxsZRRQiwdSFJdYsjbisbP%2FXnzYQDcutT54eaOfOpDClPGyPmcUexns9Eja1s5A2I8Cg4IdyX%2B%2BDkVdOKr%2BB5pjxITAaeLzMYan46wLGCXQFwV4lArKUziSFw7RuCHjE6houiC5%2FdmG%2FEQTHgr5de5aqyYvaP2rg69heR%2BRX6kvlXls9XFgInGU9mHjOyrgde2odQA1fpl422kA%3D%3D&Expires=1754373880\n",
      "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 3.5.28.250, 3.5.28.158, 52.216.179.43, ...\n",
      "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|3.5.28.250|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1414979 (1.3M) [application/x-tar]\n",
      "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
      "\n",
      "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  1.24MB/s    in 1.1s    \n",
      "\n",
      "2025-08-05 05:34:43 (1.24 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
      "\n",
      "Done\n",
      "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
      "Done\n",
      "Change Directory to mecab-0.996-ko-0.9.2.......\n",
      "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
      "configure\n",
      "make\n",
      "make check\n",
      "make install\n",
      "ldconfig\n",
      "Done\n",
      "Change Directory to /content\n",
      "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
      "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
      "--2025-08-05 05:37:34--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
      "Resolving bitbucket.org (bitbucket.org)... 104.192.143.21, 104.192.143.23, 104.192.143.22, ...\n",
      "Connecting to bitbucket.org (bitbucket.org)|104.192.143.21|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNELTC4VRI&Signature=SklAGqzG90nFL66hv21y6LJuZYc%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEB4aCXVzLWVhc3QtMSJHMEUCICYNmlhmIzYOxCDBlAbcw3aNqQButDHTMh%2FDtkNSfxehAiEA2%2FqxcZqN%2BRAJlMIIeobSXv4axtcY9hBabzDfmLqQaP8qpwIIVxAAGgw5ODQ1MjUxMDExNDYiDM5SWowGrLc7qs6AJSqEAqvQk3f%2FJ10TIYy3nvN459JeopBMlIrPaLA%2F77zwPVM8o9wj31QGWG3YgsNonDA1SdyB5UAf51TN6H4ss0gPkBK4ZcI5%2FzGbsk3nI1yXIHow0%2FkRicODlue96p7LLYTQ4xkMuo3Bse%2Fl%2FKHpImWBMdVYTqxOtjGlpvlJo6us%2BEAuy8t0xH3YELhlMqcTCFzcT5T4r8FuPqHK%2B66OUI0131%2BW8IiOccRf5rlOuGCe%2BjYpY22TCXQRN7%2BKNuAY55piAvtJ1%2Fepz%2FR42aSHVSMsu6ETOJKlghYsSiMma83x0n5SI71u1FxuEcrAQAePDO0rowhX2uVCcJBaP7wZItb1TjonnA%2BFMJ%2BuxsQGOp0BfLWWYz6APHmYltiNzJtkiLJDQPyUENfYD4ITE0m%2BzmfsHx4l%2FRpVizEW5muV%2FRHN%2FZHcGFjKWtvWAGnlPfLJZfLQ8RbyQEzBGxE3aJ77XYJXhmdnc%2Bc94By2M9H%2BmG4B7x3N%2BLwG9SbYerjMT8m%2F6KL%2FzeJIwwGcPrArd6rO%2FcMdbpubkw1jP5hdEm08zLDAoUGxjzqmWVdzaw72PQ%3D%3D&Expires=1754373671 [following]\n",
      "--2025-08-05 05:37:35--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNELTC4VRI&Signature=SklAGqzG90nFL66hv21y6LJuZYc%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEB4aCXVzLWVhc3QtMSJHMEUCICYNmlhmIzYOxCDBlAbcw3aNqQButDHTMh%2FDtkNSfxehAiEA2%2FqxcZqN%2BRAJlMIIeobSXv4axtcY9hBabzDfmLqQaP8qpwIIVxAAGgw5ODQ1MjUxMDExNDYiDM5SWowGrLc7qs6AJSqEAqvQk3f%2FJ10TIYy3nvN459JeopBMlIrPaLA%2F77zwPVM8o9wj31QGWG3YgsNonDA1SdyB5UAf51TN6H4ss0gPkBK4ZcI5%2FzGbsk3nI1yXIHow0%2FkRicODlue96p7LLYTQ4xkMuo3Bse%2Fl%2FKHpImWBMdVYTqxOtjGlpvlJo6us%2BEAuy8t0xH3YELhlMqcTCFzcT5T4r8FuPqHK%2B66OUI0131%2BW8IiOccRf5rlOuGCe%2BjYpY22TCXQRN7%2BKNuAY55piAvtJ1%2Fepz%2FR42aSHVSMsu6ETOJKlghYsSiMma83x0n5SI71u1FxuEcrAQAePDO0rowhX2uVCcJBaP7wZItb1TjonnA%2BFMJ%2BuxsQGOp0BfLWWYz6APHmYltiNzJtkiLJDQPyUENfYD4ITE0m%2BzmfsHx4l%2FRpVizEW5muV%2FRHN%2FZHcGFjKWtvWAGnlPfLJZfLQ8RbyQEzBGxE3aJ77XYJXhmdnc%2Bc94By2M9H%2BmG4B7x3N%2BLwG9SbYerjMT8m%2F6KL%2FzeJIwwGcPrArd6rO%2FcMdbpubkw1jP5hdEm08zLDAoUGxjzqmWVdzaw72PQ%3D%3D&Expires=1754373671\n",
      "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.222.25, 16.15.193.106, 52.217.46.108, ...\n",
      "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.222.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 49775061 (47M) [application/x-tar]\n",
      "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
      "\n",
      "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  13.0MB/s    in 4.2s    \n",
      "\n",
      "2025-08-05 05:37:40 (11.4 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
      "\n",
      "Done\n",
      "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
      "Done\n",
      "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
      "Done\n",
      "installing........\n",
      "configure\n",
      "make\n",
      "make install\n",
      "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/v0.6.0/scripts/mecab.sh)\n",
      "https://github.com/konlpy/konlpy/issues/395#issue-1099168405 - 2022.01.11\n",
      "Done\n",
      "Install mecab-python\n",
      "Successfully Installed\n",
      "Now you can use Mecab\n",
      "from konlpy.tag import Mecab\n",
      "mecab = Mecab()\n",
      "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
      "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
      "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n",
      "light 버전 작성 : Dogdriip님 ( https://github.com/Dogdriip )\n",
      "문제를 해결해주신 combacsa님 감사합니다.\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "%cd Mecab-ko-for-Google-Colab/\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51530069-147b-4eac-b2f8-0f2c73036340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.11\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc8fbc1-66a0-4d8a-b8ce-1e27aacb7a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==4.3.2 in /opt/conda/lib/python3.12/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.12/site-packages (from gensim==4.3.2) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from gensim==4.3.2) (1.12.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /opt/conda/lib/python3.12/site-packages (from gensim==4.3.2) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.12/site-packages (from smart_open>=1.8.1->gensim==4.3.2) (1.17.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==4.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b8cf97-42e2-4655-88c9-8d23bbef3eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy==1.12.0 in /opt/conda/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy==1.26.3 in /opt/conda/lib/python3.12/site-packages (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "# 의존성 연결을 위해 다운그레이드를 진행합니다.\n",
    "!pip install scipy==1.12.0 numpy==1.26.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45fdb15a-6ee0-4c27-95c7-a725e29ff11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "0.6.0\n",
      "4.3.2\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import konlpy\n",
    "import gensim\n",
    "\n",
    "print(pandas.__version__)\n",
    "print(konlpy.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c1c150-79d1-40f6-a92d-5524ff6e9540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 준비 및 확인\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# train_data = pd.read_table(os.getenv(\"HOME\") + '/work/sentiment_classification/data/ratings_train.txt')\n",
    "# test_data = pd.read_table(os.getenv(\"HOME\") + '/work/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "# train_data.head()\n",
    "\n",
    "\n",
    "data_path = os.path.join(os.getenv(\"HOME\"), 'work', 'workplace', 'AIFFEL_quest_rs', 'Exploration', 'Quest05', 'sentiment_classification', 'data')\n",
    "\n",
    "train_data = pd.read_table(os.path.join(data_path, 'ratings_train.txt'))\n",
    "test_data = pd.read_table(os.path.join(data_path, 'ratings_test.txt'))\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7d66d0a-c432-42a8-92e5-73de23e908a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The MeCab dictionary does not exist at \"/usr/local/lib/mecab/dic/mecab-ko-dic\". Is the dictionary correctly installed?\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab('/some/dic/path')\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/MeCab/__init__.py:137\u001b[39m, in \u001b[36mTagger.__init__\u001b[39m\u001b[34m(self, rawargs)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ee:\n",
      "\u001b[31mRuntimeError\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/konlpy/tag/_mecab.py:77\u001b[39m, in \u001b[36mMecab.__init__\u001b[39m\u001b[34m(self, dicpath)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28mself\u001b[39m.tagger = \u001b[43mTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-d \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mdicpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mself\u001b[39m.tagset = utils.read_json(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m/data/tagset/mecab.json\u001b[39m\u001b[33m'\u001b[39m % utils.installpath)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/MeCab/__init__.py:139\u001b[39m, in \u001b[36mTagger.__init__\u001b[39m\u001b[34m(self, rawargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ee:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_info(rawargs)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mee\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: \n----------------------------------------------------------\n\nFailed initializing MeCab. Please see the README for possible solutions:\n\n    https://github.com/SamuraiT/mecab-python3#common-issues\n\nIf you are still having trouble, please file an issue here, and include the\nERROR DETAILS below:\n\n    https://github.com/SamuraiT/mecab-python3/issues\n\nissueを英語で書く必要はありません。\n\n------------------- ERROR DETAILS ------------------------\narguments: -d /usr/local/lib/mecab/dic/mecab-ko-dic\ndefault dictionary path: None\n[ifs] no such file or directory: /usr/local/etc/mecabrc\n----------------------------------------------------------\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokenizer = \u001b[43mMecab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m stopwords = [\u001b[33m'\u001b[39m\u001b[33m의\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m가\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m이\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m은\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m들\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m는\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m좀\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m잘\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m걍\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m과\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m도\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m를\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m으로\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m자\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m에\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m와\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m한\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m하다\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_data\u001b[39m(train_data, test_data, num_words=\u001b[32m10000\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/konlpy/tag/_mecab.py:80\u001b[39m, in \u001b[36mMecab.__init__\u001b[39m\u001b[34m(self, dicpath)\u001b[39m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mself\u001b[39m.tagset = utils.read_json(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m/data/tagset/mecab.json\u001b[39m\u001b[33m'\u001b[39m % utils.installpath)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mThe MeCab dictionary does not exist at \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m. Is the dictionary correctly installed?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mYou can also try entering the dictionary path when initializing the Mecab class: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMecab(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m/some/dic/path\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m % dicpath)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mInstall MeCab in order to use it: http://konlpy.org/en/latest/install/\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: The MeCab dictionary does not exist at \"/usr/local/lib/mecab/dic/mecab-ko-dic\". Is the dictionary correctly installed?\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab('/some/dic/path')\""
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any')\n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any')\n",
    "\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['', '', '', ''] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41232dce-5a0c-427d-ac69-b2102fa55f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 전처리\n",
    " #데이터 중복 제거\n",
    " #데이터 Non값 제거\n",
    " #한국어 토크나이저로 토큰화\n",
    " #불용어 제거 \n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    # MeCab 설치 스크립트가 설치한 mecabrc 설정 파일의 경로를 환경 변수에 설정\n",
    "    # 이 파일은 일반적으로 설치 스크립트가 다운로드한 소스 폴더 안에 있습니다.\n",
    "    os.environ['MECABRC'] = os.path.join(os.getenv(\"HOME\"), 'work', 'workplace', 'AIFFEL_quest_rs', 'Exploration', 'Quest05', 'Mecab-ko-for-Google-Colab', 'mecab-0.996-ko-0.9.2', 'etc', 'mecabrc')\n",
    "    \n",
    "    # MeCab 사전의 실제 경로도 고객님께서 알려주신 설치 경로로 재지정합니다.\n",
    "    # 설치 스크립트가 'Mecab-ko-for-Google-Colab' 폴더 안에 사전을 생성한 것으로 추정합니다.\n",
    "    base_path = os.path.join(os.getenv(\"HOME\"), 'work', 'workplace', 'AIFFEL_quest_rs', 'Exploration', 'Quest05', 'Mecab-ko-for-Google-Colab')\n",
    "    mecab_dic_path = os.path.join(base_path, 'mecab-ko-dic')\n",
    "    \n",
    "    # 구성된 경로를 사용하여 Mecab 객체를 생성합니다.\n",
    "    tokenizer = Mecab(dicpath=mecab_dic_path)\n",
    "    print(\"Mecab 토크나이저가 성공적으로 초기화되었습니다.\")\n",
    "except Exception as e:\n",
    "    # 경로가 올바르지 않으면 오류를 출력합니다.\n",
    "    raise Exception(f\"MeCab dictionary not found or mecabrc file is missing. Please check your installation. The dictionary path attempted was: {mecab_dic_path}\")\n",
    "\n",
    "# tokenizer = Mecab()\n",
    "#tokenizer = Mecab(dicpath='/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "#데이터 클리닝\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any')\n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any')\n",
    "\n",
    "#토큰화 불용 제거 \n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    # vocab 구\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['', '', '', ''] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d1e310-265b-4839-8ba0-f7915bd7e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeca69e-191a-4063-9768-78ea161d3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다.\n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다.\n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다.\n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다.\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다.\n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16e284-ee93-4687-bf6a-fdd51ce25aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수가 정상 작동하는지 확인하기 위한 테스트 코드\n",
    "\n",
    "# 테스트를 위한 가상의 word_to_index와 index_to_word 딕셔너리를 만듭니다.\n",
    "# 실제 데이터의 딕셔너리와 구조가 유사해야 합니다.\n",
    "test_word_to_index = {\n",
    "    '<PAD>': 0, '<BOS>': 1, '<UNK>': 2, '': 3,\n",
    "    'hello': 4, 'world': 5, 'python': 6, 'is': 7, 'great': 8\n",
    "}\n",
    "test_index_to_word = {index: word for word, index in test_word_to_index.items()}\n",
    "\n",
    "# 테스트 문장\n",
    "test_sentence = 'hello world python is great'\n",
    "test_sentences = ['hello world', 'python is great', 'hello python']\n",
    "\n",
    "# 1. get_encoded_sentence 테스트\n",
    "encoded_sentence = get_encoded_sentence(test_sentence, test_word_to_index)\n",
    "print(f\"원본 문장: {test_sentence}\")\n",
    "print(f\"인코딩 결과: {encoded_sentence}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 2. get_encoded_sentences 테스트\n",
    "encoded_sentences = get_encoded_sentences(test_sentences, test_word_to_index)\n",
    "print(f\"원본 문장 리스트: {test_sentences}\")\n",
    "print(f\"인코딩 결과 리스트: {encoded_sentences}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 3. get_decoded_sentence 테스트\n",
    "decoded_sentence = get_decoded_sentence(encoded_sentence, test_index_to_word)\n",
    "print(f\"인코딩된 문장: {encoded_sentence}\")\n",
    "print(f\"디코딩 결과: {decoded_sentence}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 4. get_decoded_sentences 테스트\n",
    "decoded_sentences = get_decoded_sentences(encoded_sentences, test_index_to_word)\n",
    "print(f\"인코딩된 문장 리스트: {encoded_sentences}\")\n",
    "print(f\"디코딩 결과 리스트: {decoded_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274228e-0302-4d27-a11c-a9728cd3bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 모델 구성을 위한 데이터 분석 및 가공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46b312-02d7-47bf-bdd6-a3230505e678",
   "metadata": {},
   "outputs": [],
   "source": [
    " #데이터셋 내 문장 길이 분포\n",
    " #적절한 최대 문장 길이 지정\n",
    "\n",
    "\n",
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다.\n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print(\n",
    "    f'''전체 문장의 \n",
    "    {np.sum(num_tokens < max_tokens) / len(num_tokens)}%가 \n",
    "    maxlen 설정값 이내에 포함됩니다. ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12212b-b8be-4859-99e0-3b85c78f9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 길이 통일(패딩추가) 및 텐서 변환\n",
    "import torch  # 딥러닝 프레임워크 PyTorch 라이브러리\n",
    "\n",
    "def pad_sequences(data, maxlen):\n",
    "    padded_data = []\n",
    "    for sentence in data:\n",
    "        if len(sentence) < maxlen:\n",
    "            # 문장 길이가 maxlen보다 짧으면, 0(패딩)을 추가하여 길이 맞추기\n",
    "            sentence = sentence + [0] * (maxlen - len(sentence))\n",
    "        else:\n",
    "            # 문장 길이가 maxlen보다 길면, 뒷부분을 잘라서 길이 맞추기\n",
    "            sentence = sentence[:maxlen]\n",
    "        padded_data.append(sentence)\n",
    "    return np.array(padded_data)  # 결과를 numpy 배열로 반환\n",
    "\n",
    "X_train_padded = pad_sequences(X_train, maxlen)  # 훈련 데이터의 길이를 maxlen에 맞춤\n",
    "X_test_padded = pad_sequences(X_test, maxlen)  # 테스트 데이터의 길이를 maxlen에 맞춤\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_padded, dtype=torch.long)  # 패딩된 훈련 데이터를 PyTorch 텐서로 변환\n",
    "X_test_tensor = torch.tensor(X_test_padded, dtype=torch.long)  # 패딩된 테스트 데이터를 PyTorch 텐서로 변환\n",
    "\n",
    "print(X_train_tensor.shape)  # 변환된 훈련 텐서의 형태(shape) 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89aa2e2-3d51-4765-a690-170a87b8d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4)모델 구성 및 validation set 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cad8be-1148-4a61-a77e-1e1d0f9088e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1_1 LSTM 기반 감성 분석 모델 정의 및 구조 출력\n",
    "import torch  # 딥러닝 프레임워크 PyTorch 라이브러리\n",
    "import torch.nn as nn  # 신경망 모듈\n",
    "import torch.nn.functional as F  # 신경망 함수 모음\n",
    "\n",
    "vocab_size = 10000  # 모델이 다룰 수 있는 단어의 총 개수\n",
    "word_vector_dim = 16  # 각 단어를 표현하는 워드 벡터의 차원\n",
    "\n",
    "# 모델 설계: nn.Module을 상속받아 SentimentModel 클래스 정의\n",
    "class SentimentModel(nn.Module):\n",
    "    # 모델의 레이어(Layer)들을 정의\n",
    "    def __init__(self, vocab_size, word_vector_dim):\n",
    "        super(SentimentModel, self).__init__()  # nn.Module의 생성자 호출\n",
    "        self.embedding = nn.Embedding(vocab_size, word_vector_dim)  # 단어를 워드 벡터로 변환하는 임베딩 레이어\n",
    "        self.lstm = nn.LSTM(word_vector_dim, 8, batch_first=True)  # 워드 벡터 시퀀스를 처리하는 LSTM 레이어\n",
    "        self.fc1 = nn.Linear(8, 8)  # LSTM의 최종 출력을 받아서 처리하는 첫 번째 완전 연결 레이어\n",
    "        self.fc2 = nn.Linear(8, 1)  # 최종 출력을 0과 1 사이의 값으로 만드는 두 번째 완전 연결 레이어\n",
    "\n",
    "    # 모델의 순전파(forward) 로직을 정의\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # 입력(단어 인덱스)을 임베딩 레이어에 통과시켜 워드 벡터로 변환\n",
    "        x, (hn, cn) = self.lstm(x)  # 임베딩 벡터 시퀀스를 LSTM에 통과\n",
    "        x = x[:, -1, :]  # LSTM의 최종 시점(sequence의 마지막)의 은닉 상태를 가져옴\n",
    "        x = F.relu(self.fc1(x))  # 첫 번째 완전 연결 레이어에 통과시키고 ReLU 활성화 함수 적용\n",
    "        x = torch.sigmoid(self.fc2(x))  # 두 번째 완전 연결 레이어에 통과시키고 시그모이드 활성화 함수 적용 (0~1 사이 값)\n",
    "        return x  # 최종 출력값 반환\n",
    "\n",
    "model_lstm_random = SentimentModel(vocab_size, word_vector_dim)  # 정의된 클래스를 바탕으로 모델 객체 생성\n",
    "print(model_lstm_random)  # 모델의 레이어 구조와 파라미터 정보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86299bb4-8764-494e-96e4-3551fae16196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set 10000건 분리\n",
    "X_val = X_train_tensor[:10000]\n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_X_train = X_train_tensor[10000:]\n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_X_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8227e-3877-4e26-9fe9-9dcd2a35e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) 모델 훈련 개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b75bf-14e2-480d-9536-a21213f2229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-1 LSTM을 이용한 감성 분석 모델 훈련\n",
    "import torch.optim as optim  # 최적화 도구 모음\n",
    "import torch.nn.functional as F  # 활성화 함수, 손실 함수 등\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 데이터 로딩 유틸리티\n",
    "\n",
    "optimizer = optim.Adam(model_lstm_random.parameters())  # 모델의 파라미터를 업데이트할 Adam 최적화 도구 설정\n",
    "loss_fn = torch.nn.BCELoss()  # 이진 분류에 적합한 Binary Cross Entropy 손실 함수 설정\n",
    "\n",
    "partial_X_train_tensor = partial_X_train.detach().clone().to(torch.long)  # 훈련용 입력 텐서를 복사하고 데이터 타입을 long으로 변환\n",
    "partial_y_train_tensor = torch.tensor(partial_y_train, dtype=torch.float)  # 훈련용 라벨 데이터를 PyTorch 텐서로 변환 (BCELoss를 위해 float 타입 사용)\n",
    "\n",
    "X_val_tensor = X_val.detach().clone().to(torch.long)  # 검증용 입력 텐서를 복사하고 데이터 타입을 long으로 변환\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float)  # 검증용 라벨 데이터를 PyTorch 텐서로 변환\n",
    "\n",
    "train_dataset = TensorDataset(partial_X_train_tensor, partial_y_train_tensor)  # 훈련 데이터와 라벨을 묶어 데이터셋 생성\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)  # 검증 데이터와 라벨을 묶어 데이터셋 생성\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)  # 훈련용 데이터로더 생성 (배치 크기 512, 데이터 섞기)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)  # 검증용 데이터로더 생성 (배치 크기 512, 데이터 섞지 않음)\n",
    "\n",
    "epochs = 30  # 훈련 반복 횟수 설정\n",
    "train_losses = []  # 에포크별 훈련 손실을 저장할 리스트\n",
    "val_losses = []  # 에포크별 검증 손실을 저장할 리스트\n",
    "train_accs = []  # 에포크별 훈련 정확도를 저장할 리스트\n",
    "val_accs = []  # 에포크별 검증 정확도를 저장할 리스트\n",
    "\n",
    "# 조기 종료 관련 하이퍼파라미터\n",
    "patience = 5          # 검증 손실이 5번 연속으로 개선되지 않으면 훈련 중단\n",
    "min_delta = 0.0001    # 최소한 이 값 이상 손실이 감소해야 개선으로 인정\n",
    "\n",
    "best_loss = np.Inf    # 초기 최저 손실값 설정\n",
    "patience_counter = 0  # 손실 개선 실패 횟수 카운터\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 훈련 단계 시작\n",
    "    model_lstm_random.train()  # 모델을 훈련 모드로 설정\n",
    "    running_loss = 0.0  # 현재 에포크의 누적 손실 초기화\n",
    "    correct = 0  # 현재 에포크의 정답 수 초기화\n",
    "    total = 0  # 현재 에포크의 전체 데이터 수 초기화\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # 이전 스텝에서 계산된 기울기 초기화\n",
    "\n",
    "        outputs = model_lstm_random(inputs)  # 모델에 입력 데이터를 넣어 예측값 계산\n",
    "        loss = loss_fn(outputs.squeeze(), labels)  # 예측값과 실제 라벨 간의 손실 계산\n",
    "        loss.backward()  # 손실에 대한 기울기 역전파\n",
    "        optimizer.step()  # 모델의 파라미터 업데이트\n",
    "\n",
    "        running_loss += loss.item()  # 현재 배치 손실을 누적\n",
    "        predicted = (outputs.squeeze() > 0.5).float()  # 예측값이 0.5보다 크면 긍정(1), 아니면 부정(0)으로 판단\n",
    "        correct += (predicted == labels).sum().item()  # 맞춘 개수 누적\n",
    "        total += labels.size(0)  # 전체 데이터 수 누적\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))  # 에포크별 평균 훈련 손실 기록\n",
    "    train_accs.append(correct / total)  # 에포크별 훈련 정확도 기록\n",
    "\n",
    "    # 검증 단계 시작\n",
    "    model_lstm_random.eval()  # 모델을 평가 모드로 설정\n",
    "    val_loss = 0.0  # 현재 에포크의 누적 검증 손실 초기화\n",
    "    val_correct = 0  # 현재 에포크의 정답 수 초기화\n",
    "    val_total = 0  # 현재 에포크의 전체 데이터 수 초기화\n",
    "\n",
    "    with torch.no_grad():  # 기울기 계산을 비활성화 (메모리 절약, 연산 속도 향상)\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model_lstm_random(inputs)  # 모델에 입력 데이터를 넣어 예측값 계산\n",
    "            loss = loss_fn(outputs.squeeze(), labels)  # 손실 계산\n",
    "\n",
    "            val_loss += loss.item()  # 현재 배치 손실을 누적\n",
    "            predicted = (outputs.squeeze() > 0.5).float()  # 예측값 기반으로 정답 판단\n",
    "            val_correct += (predicted == labels).sum().item()  # 맞춘 개수 누적\n",
    "            val_total += labels.size(0)  # 전체 데이터 수 누적\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # 에포크별 평균 검증 손실 기록\n",
    "    val_accs.append(val_correct / val_total)  # 에포크별 검증 정확도 기록\n",
    "\n",
    "    # 손실 개선 여부 확인\n",
    "    if val_loss < best_loss - min_delta:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # 최적의 모델 가중치 저장\n",
    "        torch.save(model_lstm_random.state_dict(), 'model_lstm_random.pth')\n",
    "        print(f\"Validation loss improved. Saving model weights...\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Validation loss did not improve. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "    # 조기 종료 조건 확인\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        break\n",
    "\n",
    "    \n",
    "    # 에포크별 결과 출력\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"T_Loss: {train_losses[-1]:.4f}, T_Acc: {train_accs[-1]:.4f} - \"\n",
    "          f\"V_Loss: {val_losses[-1]:.4f}, V_Acc: {val_accs[-1]:.4f}\")\n",
    "\n",
    "#-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7653cda-9fa5-46c5-a21c-2aa689c935ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim 형식으로 저장할 파일 경로\n",
    "word2vec_file_path = 'model_lstm_random.txt'\n",
    "\n",
    "# 모델의 임베딩 가중치와 단어-인덱스 매핑 준비\n",
    "# 1. 모델 변수명을 'model_lstm_cnn_word2vec'로 수정\n",
    "vectors = model_lstm_random.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "# 2. 모델 훈련 시 사용했던 실제 'word_to_index' 딕셔너리를 활용하여\n",
    "#    'index_to_word' 딕셔너리를 생성합니다.\n",
    "#    (이 부분은 실제 훈련 코드에 있는 'word_to_index' 변수명으로 교체해야 합니다.)\n",
    "#    예시: word_to_index = {'<pad>': 0, '<unk>': 1, '영화': 2, ...}\n",
    "#    - word_to_index 변수가 없으면 이 코드는 에러를 발생시킵니다.\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "\n",
    "# 파일에 쓰기\n",
    "with open(word2vec_file_path, 'w') as f:\n",
    "    # 1. 파일의 첫 줄에 '전체 단어 수', '벡터 차원'을 기록합니다.\n",
    "    #    gensim이 이 정보를 보고 파일 형식을 판단합니다.\n",
    "    vocab_size, word_vector_dim = vectors.shape\n",
    "    f.write(f'{vocab_size} {word_vector_dim}\\n')\n",
    "    \n",
    "    # 2. 각 단어의 벡터 값을 공백으로 구분하여 한 줄씩 기록합니다.\n",
    "    for i in range(vocab_size):\n",
    "        # 3. index_to_word.get(i, '<unk>')를 사용하여 없는 인덱스도 안전하게 처리\n",
    "        word = index_to_word.get(i, '<unk>')\n",
    "        vector_str = ' '.join(map(str, vectors[i, :]))\n",
    "        f.write(f'{word} {vector_str}\\n')\n",
    "\n",
    "print(f\"'{word2vec_file_path}' 파일에 임베딩 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7c321-ce78-40a2-9b8b-875d5d5dfeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "model_lstm_random.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model_lstm_random(inputs)\n",
    "        loss = loss_fn(outputs.squeeze(), labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_correct / test_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b8fec-29c5-4621-8140-14b3795d5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs_range, train_losses, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs_range, val_losses, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083e088-afc8-4ba9-90b8-ecf53562acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs_range, train_accs, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs_range, val_accs, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9cef3-71ee-42a2-8923-70d059838900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8) 한국어 Word2vec 임베딩 활용하여 성능 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a176d-0e57-4ee8-83e4-7dfeb6dcf1bb",
   "metadata": {},
   "source": [
    "한국어 Word2Vec은 /data 폴더 안에 있는 word2vec_ko.model을 활용하세요.\n",
    "한국어 Word2Vec을 활용할 때는 load_word2vec_format() 형태가 아닌 load() 형태로 모델을 불러와주세요. 또한 모델을 활용할 때에는 아래 예시와 같이 .wv를 붙여서 활용합니다. 좀더 자세한 활용법에 대해선 다음 링크들을 참조해주세요. 참고 링크1, 참고 링크2\n",
    "# 예시 코드\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "word_vectors = Word2VecKeyedVectors.load(word2vec_file_path)\n",
    "vector = word_vectors.wv[‘끝’]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea3ae0-9273-4603-8431-9da1b7929aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8dfcfc-c119-4cb4-86c5-59b68053288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p sentiment_classification/data\n",
    "# !pip install gensim==4.3.2\n",
    "# !pip install scipy==1.12.0 numpy==1.26.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68aa67-dd4d-421a-a551-d746fcb271c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = model.embedding\n",
    "# weights = embedding_layer.weight.detach().cpu().numpy()\n",
    "\n",
    "# print(weights.shape)  # shape: (vocab_size, embedding_dim)\n",
    "\n",
    "\n",
    "#!mkdir ~/work/sentiment_classification/word2vec_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6762d-e3bb-4f93-9b50-f88f5cf0f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 경로를 환경 변수와 결합하여 전체 경로를 만듭니다.\n",
    "word2vec_file_path = os.path.join(os.getenv('HOME'), 'work/sentiment_classification/data/word2vec_ko.model')\n",
    "\n",
    "# load() 함수를 사용하여 한국어 Word2Vec 모델 불러오기\n",
    "try:\n",
    "    word2vec_ko = Word2Vec.load(word2vec_file_path)\n",
    "    print(\"한국어 Word2Vec 모델을 성공적으로 로드했습니다.\")\n",
    "\n",
    "    # .wv를 붙여서 활용\n",
    "    vector = word2vec_ko.wv['사랑']\n",
    "    print(f\"'사랑' 단어의 벡터 차원: {vector.shape}\")\n",
    "    print(f\"'사랑'과 유사한 단어: {word2vec_ko.wv.similar_by_word('사랑')}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: {word2vec_file_path} 경로에 파일이 존재하지 않습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"모델 로드 중 오류가 발생했습니다: {e}\")\n",
    "# 전체 워드 벡터의 차원 확인\n",
    "print(f\"전체 워드 벡터의 shape: {word2vec_ko.wv.vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7b424-00ff-4ed9-883f-6875e45cb0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 모델의 워드 벡터를 embedding_matrix에 복사\n",
    "vocab_size = 10000  # 모델이 사용할 총 단어의 개수\n",
    "#word_vector_dim = 100  # 워드 벡터의 차원 \n",
    "\n",
    "# Word2Vec 모델의 벡터 차원 확인\n",
    "word_vector_dim = word2vec_ko.wv.vector_size  # Word2Vec 모델의 실제 차원을 사용\n",
    "print(f\"Word2Vec 모델의 워드 벡터 차원: {word_vector_dim}\")\n",
    "\n",
    "#embedding_matrix = np.random.rand(vocab_size, word_vector_dim)  # 임의의 값으로 초기화된 임베딩 행렬 생성\n",
    "embedding_matrix = np.zeros((vocab_size, word_vector_dim))\n",
    "\n",
    "\n",
    "# # embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "# for i in range(4, vocab_size):  # 단어 인덱스 4부터 시작하여 모든 단어에 대해 반복\n",
    "#     if index_to_word[i] in word2vec_ko.wv:  # 현재 단어가 Word2Vec 모델에 포함되어 있는지 확인\n",
    "#         embedding_matrix[i] = word2vec_ko.wv[index_to_word[i]]  \n",
    "#         # 포함되어 있다면, 해당 단어의 워드 벡터를 embedding_matrix의 i번째 행에 복사\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # embedding_matrix에 Word2Vec 워드 벡터를 복사\n",
    "for i in range(4, vocab_size):\n",
    "    word = index_to_word[i]\n",
    "    if word in word2vec_ko.wv:\n",
    "        embedding_matrix[i] = word2vec_ko.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4549f-e592-469e-bf9d-67bd3cd8ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1_2 LSTM 기반 감성 분석 모델 정의 및 구조 출력\n",
    "import torch  # 딥러닝 프레임워크 PyTorch 라이브러리\n",
    "import torch.nn as nn  # 신경망 모듈\n",
    "import torch.nn.functional as F  # 신경망 함수 모음\n",
    "\n",
    "vocab_size = 10000  # 모델이 다룰 수 있는 단어의 총 개수\n",
    "word_vector_dim = 100  # 각 단어를 표현하는 워드 벡터의 차원\n",
    "\n",
    "# 모델 설계: nn.Module을 상속받아 SentimentModel 클래스 정의\n",
    "class SentimentModel(nn.Module):\n",
    "    # 모델의 레이어(Layer)들을 정의\n",
    "    def __init__(self, vocab_size, word_vector_dim):\n",
    "        super(SentimentModel, self).__init__()  # nn.Module의 생성자 호출\n",
    "        self.embedding = nn.Embedding(vocab_size, word_vector_dim)  # 단어를 워드 벡터로 변환하는 임베딩 레이어\n",
    "        self.lstm = nn.LSTM(word_vector_dim, 8, batch_first=True)  # 워드 벡터 시퀀스를 처리하는 LSTM 레이어\n",
    "        self.fc1 = nn.Linear(8, 8)  # LSTM의 최종 출력을 받아서 처리하는 첫 번째 완전 연결 레이어\n",
    "        self.fc2 = nn.Linear(8, 1)  # 최종 출력을 0과 1 사이의 값으로 만드는 두 번째 완전 연결 레이어\n",
    "\n",
    "    # 모델의 순전파(forward) 로직을 정의\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # 입력(단어 인덱스)을 임베딩 레이어에 통과시켜 워드 벡터로 변환\n",
    "        x, (hn, cn) = self.lstm(x)  # 임베딩 벡터 시퀀스를 LSTM에 통과\n",
    "        x = x[:, -1, :]  # LSTM의 최종 시점(sequence의 마지막)의 은닉 상태를 가져옴\n",
    "        x = F.relu(self.fc1(x))  # 첫 번째 완전 연결 레이어에 통과시키고 ReLU 활성화 함수 적용\n",
    "        x = torch.sigmoid(self.fc2(x))  # 두 번째 완전 연결 레이어에 통과시키고 시그모이드 활성화 함수 적용 (0~1 사이 값)\n",
    "        return x  # 최종 출력값 반환\n",
    "\n",
    "model_lstm_word2vec = SentimentModel(vocab_size, word_vector_dim)  # 정의된 클래스를 바탕으로 모델 객체 생성\n",
    "print(model_lstm_word2vec)  # 모델의 레이어 구조와 파라미터 정보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69475a7-d960-4ca2-9f9f-6aa80860b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch를 이용한 감성 분석 모델 훈련\n",
    "import torch.optim as optim  # 최적화 도구 모음\n",
    "import torch.nn.functional as F  # 활성화 함수, 손실 함수 등\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 데이터 로딩 유틸리티\n",
    "\n",
    "#모델 임베딩 초기화로 Word2vec 임베딩 매트릭스 적용\n",
    "model_lstm_word2vec.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model_lstm_word2vec.parameters())  # 모델의 파라미터를 업데이트할 Adam 최적화 도구 설정\n",
    "loss_fn = torch.nn.BCELoss()  # 이진 분류에 적합한 Binary Cross Entropy 손실 함수 설정\n",
    "\n",
    "partial_X_train_tensor = partial_X_train.detach().clone().to(torch.long)  # 훈련용 입력 텐서를 복사하고 데이터 타입을 long으로 변환\n",
    "partial_y_train_tensor = torch.tensor(partial_y_train, dtype=torch.float)  # 훈련용 라벨 데이터를 PyTorch 텐서로 변환 (BCELoss를 위해 float 타입 사용)\n",
    "\n",
    "X_val_tensor = X_val.detach().clone().to(torch.long)  # 검증용 입력 텐서를 복사하고 데이터 타입을 long으로 변환\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float)  # 검증용 라벨 데이터를 PyTorch 텐서로 변환\n",
    "\n",
    "train_dataset = TensorDataset(partial_X_train_tensor, partial_y_train_tensor)  # 훈련 데이터와 라벨을 묶어 데이터셋 생성\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)  # 검증 데이터와 라벨을 묶어 데이터셋 생성\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)  # 훈련용 데이터로더 생성 (배치 크기 512, 데이터 섞기)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)  # 검증용 데이터로더 생성 (배치 크기 512, 데이터 섞지 않음)\n",
    "\n",
    "epochs = 30  # 훈련 반복 횟수 설정\n",
    "train_losses = []  # 에포크별 훈련 손실을 저장할 리스트\n",
    "val_losses = []  # 에포크별 검증 손실을 저장할 리스트\n",
    "train_accs = []  # 에포크별 훈련 정확도를 저장할 리스트\n",
    "val_accs = []  # 에포크별 검증 정확도를 저장할 리스트\n",
    "\n",
    "# 조기 종료 관련 하이퍼파라미터\n",
    "patience = 5          # 검증 손실이 5번 연속으로 개선되지 않으면 훈련 중단\n",
    "min_delta = 0.0001    # 최소한 이 값 이상 손실이 감소해야 개선으로 인정\n",
    "\n",
    "best_loss = np.Inf    # 초기 최저 손실값 설정\n",
    "patience_counter = 0  # 손실 개선 실패 횟수 카운터\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 훈련 단계 시작\n",
    "    model_lstm_word2vec.train()  # 모델을 훈련 모드로 설정\n",
    "    running_loss = 0.0  # 현재 에포크의 누적 손실 초기화\n",
    "    correct = 0  # 현재 에포크의 정답 수 초기화\n",
    "    total = 0  # 현재 에포크의 전체 데이터 수 초기화\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # 이전 스텝에서 계산된 기울기 초기화\n",
    "\n",
    "        outputs = model_lstm_word2vec(inputs)  # 모델에 입력 데이터를 넣어 예측값 계산\n",
    "        loss = loss_fn(outputs.squeeze(), labels)  # 예측값과 실제 라벨 간의 손실 계산\n",
    "        loss.backward()  # 손실에 대한 기울기 역전파\n",
    "        optimizer.step()  # 모델의 파라미터 업데이트\n",
    "\n",
    "        running_loss += loss.item()  # 현재 배치 손실을 누적\n",
    "        predicted = (outputs.squeeze() > 0.5).float()  # 예측값이 0.5보다 크면 긍정(1), 아니면 부정(0)으로 판단\n",
    "        correct += (predicted == labels).sum().item()  # 맞춘 개수 누적\n",
    "        total += labels.size(0)  # 전체 데이터 수 누적\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))  # 에포크별 평균 훈련 손실 기록\n",
    "    train_accs.append(correct / total)  # 에포크별 훈련 정확도 기록\n",
    "\n",
    "    # 검증 단계 시작\n",
    "    model_lstm_word2vec.eval()  # 모델을 평가 모드로 설정\n",
    "    val_loss = 0.0  # 현재 에포크의 누적 검증 손실 초기화\n",
    "    val_correct = 0  # 현재 에포크의 정답 수 초기화\n",
    "    val_total = 0  # 현재 에포크의 전체 데이터 수 초기화\n",
    "\n",
    "    with torch.no_grad():  # 기울기 계산을 비활성화 (메모리 절약, 연산 속도 향상)\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model_lstm_word2vec(inputs)  # 모델에 입력 데이터를 넣어 예측값 계산\n",
    "            loss = loss_fn(outputs.squeeze(), labels)  # 손실 계산\n",
    "\n",
    "            val_loss += loss.item()  # 현재 배치 손실을 누적\n",
    "            predicted = (outputs.squeeze() > 0.5).float()  # 예측값 기반으로 정답 판단\n",
    "            val_correct += (predicted == labels).sum().item()  # 맞춘 개수 누적\n",
    "            val_total += labels.size(0)  # 전체 데이터 수 누적\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))  # 에포크별 평균 검증 손실 기록\n",
    "    val_accs.append(val_correct / val_total)  # 에포크별 검증 정확도 기록\n",
    "\n",
    "    val_loss = val_losses[-1] # 현재 에포크의 검증 손실\n",
    "\n",
    "    # 손실 개선 여부 확인\n",
    "    if val_loss < best_loss - min_delta:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # 최적의 모델 가중치 저장\n",
    "        torch.save(model_lstm_word2vec.state_dict(), 'model_lstm_word2vec.pth')\n",
    "        print(f\"Validation loss improved. Saving model weights...\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Validation loss did not improve. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "    # 조기 종료 조건 확인\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        break\n",
    "    \n",
    "    # 에포크별 결과 출력\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"T_Loss: {train_losses[-1]:.4f}, T_Acc: {train_accs[-1]:.4f} - \"\n",
    "          f\"V_Loss: {val_losses[-1]:.4f}, V_Acc: {val_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31fe97-4795-447f-b995-904b6f6e7ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim 형식으로 저장할 파일 경로\n",
    "word2vec_file_path = 'model_lstm_word2vec.txt'\n",
    "\n",
    "# 모델의 임베딩 가중치와 단어-인덱스 매핑 준비\n",
    "# 1. 모델 변수명을 'model_lstm_cnn_word2vec'로 수정\n",
    "vectors = model_lstm_word2vec.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "# 2. 모델 훈련 시 사용했던 실제 'word_to_index' 딕셔너리를 활용하여\n",
    "#    'index_to_word' 딕셔너리를 생성합니다.\n",
    "#    (이 부분은 실제 훈련 코드에 있는 'word_to_index' 변수명으로 교체해야 합니다.)\n",
    "#    예시: word_to_index = {'<pad>': 0, '<unk>': 1, '영화': 2, ...}\n",
    "#    - word_to_index 변수가 없으면 이 코드는 에러를 발생시킵니다.\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "\n",
    "# 파일에 쓰기\n",
    "with open(word2vec_file_path, 'w') as f:\n",
    "    # 1. 파일의 첫 줄에 '전체 단어 수', '벡터 차원'을 기록합니다.\n",
    "    #    gensim이 이 정보를 보고 파일 형식을 판단합니다.\n",
    "    vocab_size, word_vector_dim = vectors.shape\n",
    "    f.write(f'{vocab_size} {word_vector_dim}\\n')\n",
    "    \n",
    "    # 2. 각 단어의 벡터 값을 공백으로 구분하여 한 줄씩 기록합니다.\n",
    "    for i in range(vocab_size):\n",
    "        # 3. index_to_word.get(i, '<unk>')를 사용하여 없는 인덱스도 안전하게 처리\n",
    "        word = index_to_word.get(i, '<unk>')\n",
    "        vector_str = ' '.join(map(str, vectors[i, :]))\n",
    "        f.write(f'{word} {vector_str}\\n')\n",
    "\n",
    "print(f\"'{word2vec_file_path}' 파일에 임베딩 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2dc04-9f1e-4aeb-bb33-946133c6c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "model_lstm_word2vec.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model_lstm_word2vec(inputs)\n",
    "        loss = loss_fn(outputs.squeeze(), labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_correct / test_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338ab49-4b39-4279-89c4-8c1b9ba2755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs_range, train_losses, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs_range, val_losses, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33ac68-ba80-429d-bf54-29c973656ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs_range, train_accs, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs_range, val_accs, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad1645-ef56-41a8-822c-4ac6e123fed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a624a-f43d-421e-ae67-ef8a3cb7b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff3fcf-6a18-4301-9c61-e97433dbce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2_1 1D CNN을 이용한 감성 분석 모델 정의\n",
    "import torch  # 딥러닝 프레임워크 PyTorch 라이브러리\n",
    "from torch import nn  # 신경망 모듈\n",
    "import torch.nn.functional as F  # 신경망 함수 모음 (시그모이드 함수를 위해 필요)\n",
    "\n",
    "# SentenceClassifier_Cnn 클래스 정의 (nn.Module 상속)\n",
    "class SentenceClassifier_Cnn(nn.Module):\n",
    "    # 모델의 레이어(Layer)들을 정의하는 생성자\n",
    "    def __init__(self, embedding_matrix, filter_sizes, max_length, dropout=0.5):\n",
    "        super().__init__()  # nn.Module의 생성자 호출\n",
    "\n",
    "        # 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=embedding_matrix.shape[0],\n",
    "            embedding_dim=embedding_matrix.shape[1]\n",
    "        )\n",
    "        # 외부에서 전달받은 행렬의 값을 임베딩 레이어의 가중치로 복사\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        \n",
    "        # 임베딩 레이어의 차원(word_vector_dim) 추출\n",
    "        embedding_dim = self.embedding.weight.shape[1]\n",
    "        num_filters = 128  # out_channels 수를 128로 설정\n",
    "\n",
    "        conv = []\n",
    "        for size in filter_sizes:  # filter_sizes (예: [3, 4, 5])에 따라 여러 개의 CNN 필터를 만듦\n",
    "            conv.append(\n",
    "                nn.Sequential(  # 여러 레이어를 묶어서 하나의 모듈로 구성\n",
    "                    nn.Conv1d(  # 1차원 컨볼루션 레이어\n",
    "                        in_channels=embedding_dim,  # 입력 채널 수: 워드 벡터의 차원\n",
    "                        out_channels=num_filters,  # 출력 채널 수: 각 필터에서 하나의 특징을 추출\n",
    "                        kernel_size=size  # 필터의 크기 (ex: 3, 4, 5)\n",
    "                    ),\n",
    "                    nn.ReLU(),  # 비선형성을 추가하는 활성화 함수\n",
    "                    nn.AdaptiveMaxPool1d(1),  # 최대 풀링으로 가장 중요한 특징 추출\n",
    "                )\n",
    "            )\n",
    "        self.conv_filters = nn.ModuleList(conv)  # 여러 컨볼루션 모듈을 리스트 형태로 저장\n",
    "\n",
    "        num_filters = 128\n",
    "        output_size = num_filters * len(filter_sizes) # 128 * 3 = 384 # 필터의 개수가 최종 출력의 크기가 됨\n",
    "        self.pre_classifier = nn.Linear(output_size, 64)  # 첫 번째 완전 연결 레이어\n",
    "        self.dropout = nn.Dropout(dropout)  # 과적합을 방지하는 드롭아웃 레이어\n",
    "        self.classifier = nn.Linear(64, 1)  # 최종 분류를 위한 완전 연결 레이어 (출력 1개)\n",
    "\n",
    "    # 모델의 순전파(forward) 로직을 정의\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)  # 입력(단어 인덱스)을 임베딩 레이어에 통과\n",
    "        embeddings = embeddings.permute(0, 2, 1)  # Conv1d 입력에 맞게 차원 변경 (배치, 차원, 길이)\n",
    "\n",
    "        conv_outputs = [conv(embeddings) for conv in self.conv_filters]  # 각 필터를 통과한 결과물을 리스트에 저장\n",
    "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)  # 필터 결과물을 하나로 합침\n",
    "\n",
    "        logits = self.pre_classifier(concat_outputs)  # 첫 번째 완전 연결 레이어 통과\n",
    "        logits = self.dropout(logits)  # 드롭아웃 적용\n",
    "        logits = self.classifier(logits)  # 최종 분류 레이어 통과\n",
    "        \n",
    "        return torch.sigmoid(logits)  # 이진 분류를 위해 시그모이드 활성화 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bd5b7-40c8-48c6-b6bf-ff3c83bb76a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 델 인스턴스화를 위한 변수 설정\n",
    "# 이전 LSTM 실험에서 사용한 값들을 기준으로 설정\n",
    "vocab_size = 10000 \n",
    "word_vector_dim = 16\n",
    "max_length = 41  # 예시 값 (실제 데이터의 max_length로 대체 필요)\n",
    "\n",
    "# 실험을 위한 랜덤 임베딩 행렬 생성\n",
    "# Word2Vec을 사용하지 않고 랜덤 값으로 초기화\n",
    "random_embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# 1D CNN 모델 인스턴스화 (랜덤 임베딩 사용)\n",
    "model_cnn_random = SentenceClassifier_Cnn(\n",
    "    embedding_matrix=random_embedding_matrix, \n",
    "    filter_sizes=[2, 3, 4], # 예시: 다양한 크기의 필터 사용\n",
    "    max_length=max_length,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "# 6. 모델 구조 출력\n",
    "print(\"첫 번째 모델 (랜덤 임베딩) 구조:\")\n",
    "print(model_cnn_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7989f8-9ae7-4c1d-953d-706d75c6d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2_1 1D CNN을 이용한 감성 분석 모델 훈련\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- 아래 코드는 제거해야 합니다. cnn 1 실험은 Word2Vec을 사용하지 않습니다. ---\n",
    "# model.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# model 변수에는 'model_cnn_random' 객체가 할당되어 있다고 가정\n",
    "# optimizer를 정의할 때 이 모델을 사용해야 합니다.\n",
    "optimizer = optim.Adam(model_cnn_random.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "partial_X_train_tensor = partial_X_train.detach().clone().to(torch.long)\n",
    "partial_y_train_tensor = torch.tensor(partial_y_train, dtype=torch.float)\n",
    "\n",
    "X_val_tensor = X_val.detach().clone().to(torch.long)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "train_dataset = TensorDataset(partial_X_train_tensor, partial_y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "# 조기 종료 관련 하이퍼파라미터\n",
    "patience = 5          # 검증 손실이 5번 연속으로 개선되지 않으면 훈련 중단\n",
    "min_delta = 0.0001    # 최소한 이 값 이상 손실이 감소해야 개선으로 인정\n",
    "best_loss = np.Inf    # 초기 최저 손실값 설정\n",
    "patience_counter = 0  # 손실 개선 실패 횟수 카운터\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 훈련 단계 시작\n",
    "    model_cnn_random.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_cnn_random(inputs)\n",
    "        loss = loss_fn(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accs.append(correct / total)\n",
    "\n",
    "    # 검증 단계 시작\n",
    "    model_cnn_random.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model_cnn_random(inputs)\n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "            val_loss += loss.item()\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accs.append(val_correct / val_total)\n",
    "\n",
    "    val_loss = val_losses[-1] # 현재 에포크의 검증 손실\n",
    "\n",
    "    # 손실 개선 여부 확인\n",
    "    if val_loss < best_loss - min_delta:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # 최적의 모델 가중치 저장\n",
    "        torch.save(model_cnn_random.state_dict(), 'model_cnn_random.pth')\n",
    "        print(f\"Validation loss improved. Saving model weights...\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Validation loss did not improve. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "    # 조기 종료 조건 확인\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        break\n",
    "\n",
    "\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"T_Loss: {train_losses[-1]:.4f}, T_Acc: {train_accs[-1]:.4f} - \"\n",
    "          f\"V_Loss: {val_losses[-1]:.4f}, V_Acc: {val_accs[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db15e1a-5be0-468a-939b-4eae408c2927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim 형식으로 저장할 파일 경로\n",
    "word2vec_file_path = 'model_cnn_random.txt'\n",
    "\n",
    "# 모델의 임베딩 가중치와 단어-인덱스 매핑 준비\n",
    "# 1. 모델 변수명을 'model_lstm_cnn_word2vec'로 수정\n",
    "vectors = model_cnn_random.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "# 2. 모델 훈련 시 사용했던 실제 'word_to_index' 딕셔너리를 활용하여\n",
    "#    'index_to_word' 딕셔너리를 생성합니다.\n",
    "#    (이 부분은 실제 훈련 코드에 있는 'word_to_index' 변수명으로 교체해야 합니다.)\n",
    "#    예시: word_to_index = {'<pad>': 0, '<unk>': 1, '영화': 2, ...}\n",
    "#    - word_to_index 변수가 없으면 이 코드는 에러를 발생시킵니다.\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "\n",
    "# 파일에 쓰기\n",
    "with open(word2vec_file_path, 'w') as f:\n",
    "    # 1. 파일의 첫 줄에 '전체 단어 수', '벡터 차원'을 기록합니다.\n",
    "    #    gensim이 이 정보를 보고 파일 형식을 판단합니다.\n",
    "    vocab_size, word_vector_dim = vectors.shape\n",
    "    f.write(f'{vocab_size} {word_vector_dim}\\n')\n",
    "    \n",
    "    # 2. 각 단어의 벡터 값을 공백으로 구분하여 한 줄씩 기록합니다.\n",
    "    for i in range(vocab_size):\n",
    "        # 3. index_to_word.get(i, '<unk>')를 사용하여 없는 인덱스도 안전하게 처리\n",
    "        word = index_to_word.get(i, '<unk>')\n",
    "        vector_str = ' '.join(map(str, vectors[i, :]))\n",
    "        f.write(f'{word} {vector_str}\\n')\n",
    "\n",
    "print(f\"'{word2vec_file_path}' 파일에 임베딩 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6cbb05-16b7-4d66-b7aa-d3b4b412d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "model_cnn_random.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model_cnn_random(inputs)\n",
    "        loss = loss_fn(outputs.squeeze(), labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_correct / test_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d1f6b-82ec-4668-9241-1b3381293822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs_range, train_losses, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs_range, val_losses, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb087f-7d62-4e82-bbb4-00322d3f47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs_range, train_accs, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs_range, val_accs, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb76d0-862f-4564-a1dd-528ec4932e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d34601c-6ec2-4cb2-9dbe-260202d2fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2_2 1D CNN 모델 인스턴스화를 위한 변수 설정(word2vec 임베딩 사용)\n",
    "# Word2Vec 모델의 차원에 맞춰 word_vector_dim을 100으로 설정\n",
    "vocab_size = 10000 \n",
    "word_vector_dim = 100\n",
    "max_length = 41  # pad_sequences의 maxlen 값 사용\n",
    "\n",
    "# 1. Word2Vec 모델 로드 및 임베딩 행렬 생성\n",
    "word2vec_file_path = os.path.join(os.getenv('HOME'), 'work/sentiment_classification/data/word2vec_ko.model')\n",
    "word2vec_ko = Word2Vec.load(word2vec_file_path)\n",
    "\n",
    "# word_index = ...  # 단어 인덱스 딕셔너리\n",
    "# index_to_word = {index: word for word, index in word_index.items()}\n",
    "\n",
    "# Word2Vec으로 채워진 임베딩 행렬 생성\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "for i in range(4, vocab_size):\n",
    "    if index_to_word[i] in word2vec_ko.wv:\n",
    "        embedding_matrix[i] = word2vec_ko.wv[index_to_word[i]]\n",
    "\n",
    "# 2. 1D CNN 모델 인스턴스화 (Word2Vec 임베딩 사용)\n",
    "# SentenceClassifier_Cnn 클래스는 이미 정의되어 있다고 가정\n",
    "model_cnn_word2vec = SentenceClassifier_Cnn(\n",
    "    embedding_matrix=embedding_matrix, \n",
    "    filter_sizes=[2, 3, 4],\n",
    "    max_length=max_length,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "# 모델 구조 출력 (확인용)\n",
    "print(\"--- cnn 2: Word2Vec 임베딩 행렬 사용 ---\")\n",
    "print(model_cnn_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3045a-808c-4715-9d9b-bf18a2348300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2_2 1D CNN을 이용한 감성 분석 모델 훈련2 word2vec\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 이 코드는 Word2Vec 임베딩 행렬로 모델을 초기화하는 부분입니다.\n",
    "# cnn 2 실험에 필수적이므로 그대로 유지해야 합니다.\n",
    "# model_cnn_word2vec.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "\n",
    "# model 변수에는 'model_cnn_word2ve' 객체가 할당되어 있다고 가정\n",
    "# optimizer를 정의할 때 이 모델을 사용해야 합니다.\n",
    "# --- 이 부분을 수정해야 합니다. ---\n",
    "optimizer = optim.Adam(model_cnn_word2vec.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "partial_X_train_tensor = partial_X_train.detach().clone().to(torch.long)\n",
    "partial_y_train_tensor = torch.tensor(partial_y_train, dtype=torch.float)\n",
    "\n",
    "X_val_tensor = X_val.detach().clone().to(torch.long)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "train_dataset = TensorDataset(partial_X_train_tensor, partial_y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "# 조기 종료 관련 하이퍼파라미터\n",
    "patience = 5          # 검증 손실이 5번 연속으로 개선되지 않으면 훈련 중단\n",
    "min_delta = 0.0001    # 최소한 이 값 이상 손실이 감소해야 개선으로 인정\n",
    "best_loss = np.Inf    # 초기 최저 손실값 설정\n",
    "patience_counter = 0  # 손실 개선 실패 횟수 카운터\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 훈련 단계 시작\n",
    "    # --- 이 부분도 수정해야 합니다. ---\n",
    "    model_cnn_word2vec.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # --- 이 부분도 수정해야 합니다. ---\n",
    "        outputs = model_cnn_word2vec(inputs)\n",
    "        loss = loss_fn(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accs.append(correct / total)\n",
    "\n",
    "    # 검증 단계 시작\n",
    "    model_cnn_word2vec.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model_cnn_word2vec(inputs)\n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "            val_loss += loss.item()\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accs.append(val_correct / val_total)\n",
    "\n",
    "    val_loss = val_losses[-1] # 현재 에포크의 검증 손실\n",
    "\n",
    "    # 손실 개선 여부 확인\n",
    "    if val_loss < best_loss - min_delta:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # 최적의 모델 가중치 저장\n",
    "        torch.save(model_cnn_word2vec.state_dict(), 'model_cnn_word2vec.pth')\n",
    "        print(f\"Validation loss improved. Saving model weights...\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Validation loss did not improve. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "    # 조기 종료 조건 확인\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        break\n",
    "\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"T_Loss: {train_losses[-1]:.4f}, T_Acc: {train_accs[-1]:.4f} - \"\n",
    "          f\"V_Loss: {val_losses[-1]:.4f}, V_Acc: {val_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc80a2c-0722-4c60-b017-bb6dbe33bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim 형식으로 저장할 파일 경로\n",
    "word2vec_file_path = 'model_cnn_word2vec.txt'\n",
    "\n",
    "# 모델의 임베딩 가중치와 단어-인덱스 매핑 준비\n",
    "# 1. 모델 변수명을 'model_lstm_cnn_word2vec'로 수정\n",
    "vectors = model_cnn_word2vec.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "# 2. 모델 훈련 시 사용했던 실제 'word_to_index' 딕셔너리를 활용하여\n",
    "#    'index_to_word' 딕셔너리를 생성합니다.\n",
    "#    (이 부분은 실제 훈련 코드에 있는 'word_to_index' 변수명으로 교체해야 합니다.)\n",
    "#    예시: word_to_index = {'<pad>': 0, '<unk>': 1, '영화': 2, ...}\n",
    "#    - word_to_index 변수가 없으면 이 코드는 에러를 발생시킵니다.\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "\n",
    "# 파일에 쓰기\n",
    "with open(word2vec_file_path, 'w') as f:\n",
    "    # 1. 파일의 첫 줄에 '전체 단어 수', '벡터 차원'을 기록합니다.\n",
    "    #    gensim이 이 정보를 보고 파일 형식을 판단합니다.\n",
    "    vocab_size, word_vector_dim = vectors.shape\n",
    "    f.write(f'{vocab_size} {word_vector_dim}\\n')\n",
    "    \n",
    "    # 2. 각 단어의 벡터 값을 공백으로 구분하여 한 줄씩 기록합니다.\n",
    "    for i in range(vocab_size):\n",
    "        # 3. index_to_word.get(i, '<unk>')를 사용하여 없는 인덱스도 안전하게 처리\n",
    "        word = index_to_word.get(i, '<unk>')\n",
    "        vector_str = ' '.join(map(str, vectors[i, :]))\n",
    "        f.write(f'{word} {vector_str}\\n')\n",
    "\n",
    "print(f\"'{word2vec_file_path}' 파일에 임베딩 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71543dd1-d56a-455c-8d38-c4a58bcab5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "model_cnn_word2vec.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model_cnn_word2vec(inputs)\n",
    "        loss = loss_fn(outputs.squeeze(), labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_correct / test_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f1194-4a5f-4ad6-8380-8e8cc4c28dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs_range, train_losses, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs_range, val_losses, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d5db0-3122-48fe-b064-80de1c4a702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs_range, train_accs, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs_range, val_accs, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838ae52-bf7f-4f14-b43e-60bf1b7c72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016eeafe-5d4c-4c93-934c-83127a945cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "3-1\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentenceClassifier_LstmCnn(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, num_layers, dropout, bidirectional, filter_sizes, max_length):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. 임베딩 레이어: 단어를 벡터로 변환\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=embedding_matrix.shape[0],\n",
    "            embedding_dim=embedding_matrix.shape[1]\n",
    "        )\n",
    "        \n",
    "          # 외부에서 전달받은 행렬의 값을 임베딩 레이어의 가중치로 복사\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        \n",
    "        embedding_dim = self.embedding.weight.shape[1]\n",
    "        # self.embedding.weight.requires_grad = True\n",
    "\n",
    "        # 2. LSTM 레이어: 문장의 순차적인 흐름과 맥락을 학습\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # 입력 텐서의 첫 번째 차원이 배치 크기임을 명시\n",
    "        )\n",
    "        \n",
    "        # LSTM의 출력 차원을 계산\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        num_filters = 128  # out_channels 수를 128로 설정\n",
    "        \n",
    "        # 3. CNN 필터들: LSTM 출력에서 특징을 추출\n",
    "        conv = []\n",
    "        for size in filter_sizes:\n",
    "            conv.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=lstm_output_dim,  # LSTM의 출력을 입력으로 사용\n",
    "                        out_channels=num_filters,\n",
    "                        kernel_size=size\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.AdaptiveMaxPool1d(1),\n",
    "                )\n",
    "            )\n",
    "        self.conv_filters = nn.ModuleList(conv)\n",
    "        \n",
    "        # 4. 분류기 레이어: 추출된 특징으로 최종 분류\n",
    "        output_size = num_filters * len(filter_sizes) # 128 * 3 = 384\n",
    "\n",
    "        self.pre_classifier = nn.Linear(output_size, 64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # (1) 임베딩 레이어 통과\n",
    "        embeddings = self.embedding(inputs)  # [배치, 시퀀스 길이, 임베딩 차원]\n",
    "\n",
    "        # (2) LSTM 레이어 통과\n",
    "        lstm_output, (hidden, cell) = self.lstm(embeddings)\n",
    "        # lstm_output: [배치, 시퀀스 길이, hidden_dim * 2 (양방향인 경우)]\n",
    "\n",
    "        # (3) CNN 레이어 입력에 맞게 차원 변경\n",
    "        # Conv1d를 위해 [배치, 채널 수, 시퀀스 길이] 형태로 변경\n",
    "        lstm_output = lstm_output.permute(0, 2, 1)\n",
    "\n",
    "        # (4) CNN 필터들 통과\n",
    "        conv_outputs = [conv(lstm_output) for conv in self.conv_filters]\n",
    "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
    "\n",
    "        # (5) 분류기 통과\n",
    "        logits = self.pre_classifier(concat_outputs)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.classifier(logits)\n",
    "\n",
    "        # 최종 시그모이드 활성화 함수 적용\n",
    "        return torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca456fa-9d9f-4957-9422-05151abbcb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. LSTM-CNN 모델 학습 코드의 목적\n",
    "# 이 코드는 SentenceClassifier_LstmCnn 모델을 사용하여 영화 리뷰 감성 분석 태스크를 훈련하고 평가하는 과정을 수행한다.\n",
    "# 훈련 손실(train_loss)과 정확도(train_accuracy), 그리고 검증 손실(validation_loss)과 정확도(validation_accuracy)를 매 에포크마다 계산하고 출력한다.\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim  # 최적화 도구 모음\n",
    "import torch.nn.functional as F  # 활성화 함수, 손실 함수 등\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 데이터 로딩 유틸리티\n",
    "import numpy as np\n",
    "# from SentenceClassifier_LstmCnn import SentenceClassifier_LstmCnn  # 모델 클래스 import\n",
    "from tqdm import tqdm  # tqdm 라이브러리 import\n",
    "import numpy as np\n",
    "\n",
    "# 조기 종료 관련 하이퍼파라미터\n",
    "patience = 5          # 검증 손실이 5번 연속으로 개선되지 않으면 훈련 중단\n",
    "min_delta = 0.0001    # 최소한 이 값 이상 손실이 감소해야 개선으로 인정\n",
    "\n",
    "best_loss = np.Inf    # 초기 최저 손실값 설정\n",
    "patience_counter = 0  # 손실 개선 실패 횟수 카운터\n",
    "\n",
    "\n",
    "# 모델 인스턴스화를 위한 변수 설정\n",
    "# 이전 실험들과 일관성을 유지하거나 새로운 하이퍼파라미터를 설정한다.\n",
    "vocab_size = 10000 \n",
    "word_vector_dim = 16  # Word2Vec 임베딩을 위해 100으로 설정 (랜덤 실험 시에는 16)\n",
    "max_length = 41\n",
    "hidden_dim = 128  # LSTM의 은닉 상태 차원\n",
    "num_layers = 2   # LSTM 레이어 수\n",
    "dropout = 0.5  # 드롭아웃 확률\n",
    "bidirectional = True # LSTM 양방향 설정\n",
    "filter_sizes = [2, 3, 4] # CNN 필터 크기\n",
    "\n",
    "# Word2Vec 임베딩 행렬을 사용한다고 가정하고, 이를 생성\n",
    "# (실제 Word2Vec 모델 로드 및 임베딩 행렬 생성 코드는 이전에 수행되었어야 함)\n",
    "# embedding_matrix = ...\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "\n",
    "# 모델 인스턴스화\n",
    "model_lstm_cnn_random = SentenceClassifier_LstmCnn(\n",
    "    embedding_matrix=embedding_matrix,  \n",
    "    hidden_dim=hidden_dim,  \n",
    "    num_layers=num_layers,  \n",
    "    dropout=dropout,  \n",
    "    bidirectional=bidirectional,  \n",
    "    filter_sizes=filter_sizes,  \n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "# 1. 모델 임베딩 초기화 (Word2Vec 사용 시)\n",
    "# 모델의 임베딩 레이어를 사전 학습된 Word2Vec 벡터로 덮어씌운다.\n",
    "# 랜덤 임베딩을 사용하는 실험에서는 이 줄을 주석 처리해야 한다.\n",
    "#model_lstm_cnn_random.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "\n",
    "# 2. 옵티마이저와 손실 함수 설정\n",
    "# Adam 최적화 도구와 이진 분류에 적합한 BCELoss 손실 함수를 사용한다.\n",
    "optimizer = optim.Adam(model_lstm_cnn_random.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# 3. 데이터 로더 준비\n",
    "# 훈련 및 검증 데이터를 텐서로 변환하고 데이터셋, 데이터로더를 생성한다.\n",
    "partial_X_train_tensor = partial_X_train.detach().clone().to(torch.long)\n",
    "partial_y_train_tensor = torch.tensor(partial_y_train, dtype=torch.float)\n",
    "\n",
    "X_val_tensor = X_val.detach().clone().to(torch.long)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "train_dataset = TensorDataset(partial_X_train_tensor, partial_y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "epochs = 30  # 훈련 반복 횟수 설정\n",
    "train_losses = []  # 에포크별 훈련 손실을 저장할 리스트\n",
    "val_losses = []    # 에포크별 검증 손실을 저장할 리스트\n",
    "train_accs = []    # 에포크별 훈련 정확도를 저장할 리스트\n",
    "val_accs = []      # 에포크별 검증 정확도를 저장할 리스트\n",
    "\n",
    "# 4. 모델 훈련 및 검증 루프\n",
    "for epoch in range(epochs):\n",
    "    # 훈련 단계 시작\n",
    "    model_lstm_cnn_random.train()  # 모델을 훈련 모드로 설정\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "        optimizer.zero_grad()  # 이전 스텝에서 계산된 기울기 초기화\n",
    "        outputs = model_lstm_cnn_random(inputs)  # 모델에 입력 데이터를 넣어 예측값 계산\n",
    "        loss = loss_fn(outputs.squeeze(), labels)  # 예측값과 실제 라벨 간의 손실 계산\n",
    "        loss.backward()  # 손실에 대한 기울기 역전파\n",
    "        optimizer.step()  # 모델의 파라미터 업데이트\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()  # 예측값이 0.5보다 크면 긍정(1), 아니면 부정(0)으로 판단\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accs.append(correct / total)\n",
    "\n",
    "    # 검증 단계 시작\n",
    "    model_lstm_cnn_random.eval()  # 모델을 평가 모드로 설정\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "\n",
    "    with torch.no_grad():  # 기울기 계산 비활성화 (메모리 절약, 속도 향상)\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\"):\n",
    "            outputs = model_lstm_cnn_random(inputs)\n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "            val_loss += loss.item()\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accs.append(val_correct / val_total)\n",
    "\n",
    "    val_loss = val_losses[-1] # 현재 에포크의 검증 손실\n",
    "\n",
    "    # 손실 개선 여부 확인\n",
    "    if val_loss < best_loss - min_delta:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # 최적의 모델 가중치 저장\n",
    "        torch.save(model_lstm_cnn_random.state_dict(), 'model_lstm_cnn_random.pth')\n",
    "        print(f\"Validation loss improved. Saving model weights...\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Validation loss did not improve. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "    # 조기 종료 조건 확인\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        break\n",
    "\n",
    "\n",
    "    \n",
    "    # 에포크별 결과 출력\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"T_Loss: {train_losses[-1]:.4f}, T_Acc: {train_accs[-1]:.4f} - \"\n",
    "          f\"V_Loss: {val_losses[-1]:.4f}, V_Acc: {val_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8aed8-45ae-4d2c-99a0-b36399c4f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim 형식으로 저장할 파일 경로\n",
    "word2vec_file_path = 'model_lstm_cnn_random.txt'\n",
    "\n",
    "# 모델의 임베딩 가중치와 단어-인덱스 매핑 준비\n",
    "# 1. 모델 변수명을 'model_lstm_cnn_word2vec'로 수정\n",
    "vectors = model_lstm_cnn_random.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "# 2. 모델 훈련 시 사용했던 실제 'word_to_index' 딕셔너리를 활용하여\n",
    "#    'index_to_word' 딕셔너리를 생성합니다.\n",
    "#    (이 부분은 실제 훈련 코드에 있는 'word_to_index' 변수명으로 교체해야 합니다.)\n",
    "#    예시: word_to_index = {'<pad>': 0, '<unk>': 1, '영화': 2, ...}\n",
    "#    - word_to_index 변수가 없으면 이 코드는 에러를 발생시킵니다.\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "\n",
    "# 파일에 쓰기\n",
    "with open(word2vec_file_path, 'w') as f:\n",
    "    # 1. 파일의 첫 줄에 '전체 단어 수', '벡터 차원'을 기록합니다.\n",
    "    #    gensim이 이 정보를 보고 파일 형식을 판단합니다.\n",
    "    vocab_size, word_vector_dim = vectors.shape\n",
    "    f.write(f'{vocab_size} {word_vector_dim}\\n')\n",
    "    \n",
    "    # 2. 각 단어의 벡터 값을 공백으로 구분하여 한 줄씩 기록합니다.\n",
    "    for i in range(vocab_size):\n",
    "        # 3. index_to_word.get(i, '<unk>')를 사용하여 없는 인덱스도 안전하게 처리\n",
    "        word = index_to_word.get(i, '<unk>')\n",
    "        vector_str = ' '.join(map(str, vectors[i, :]))\n",
    "        f.write(f'{word} {vector_str}\\n')\n",
    "\n",
    "print(f\"'{word2vec_file_path}' 파일에 임베딩 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9ef02-e626-4c52-8bec-ff3dd15ada08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "model_lstm_cnn_random.load_state_dict(torch.load('model_lstm_cnn_random.pth'))\n",
    "model_lstm_cnn_random.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model_lstm_cnn_random(inputs)\n",
    "        loss = loss_fn(outputs.squeeze(), labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_correct / test_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56edb0b-1812-490d-978b-b9b38472a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs_range, train_losses, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs_range, val_losses, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cea76b-f6db-4621-a99e-0fa8a88000f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs_range, train_accs, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs_range, val_accs, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea5ead-5e2e-4d88-8287-7b83557556a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "3-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837d960-e374-4b79-8f1c-6105cf6aed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-2 0. LSTM-CNN 모델 학습 코드의 목적\n",
    "# 이 코드는 SentenceClassifier_LstmCnn 모델을 사용하여 영화 리뷰 감성 분석 태스크를 훈련하고 평가하는 과정을 수행한다.\n",
    "# 훈련 손실(train_loss)과 정확도(train_accuracy), 그리고 검증 손실(validation_loss)과 정확도(validation_accuracy)를 매 에포크마다 계산하고 출력한다.\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim  # 최적화 도구 모음\n",
    "import torch.nn.functional as F  # 활성화 함수, 손실 함수 등\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 데이터 로딩 유틸리티\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # tqdm 라이브러리 import\n",
    "import numpy as np\n",
    "\n",
    "# 조기 종료 관련 하이퍼파라미터\n",
    "patience = 5          # 검증 손실이 5번 연속으로 개선되지 않으면 훈련 중단\n",
    "min_delta = 0.0001    # 최소한 이 값 이상 손실이 감소해야 개선으로 인정\n",
    "\n",
    "best_loss = np.Inf    # 초기 최저 손실값 설정\n",
    "patience_counter = 0  # 손실 개선 실패 횟수 카운터\n",
    "\n",
    "# 모델 인스턴스화를 위한 변수 설정\n",
    "# 이전 실험들과 일관성을 유지하거나 새로운 하이퍼파라미터를 설정한다.\n",
    "vocab_size = 10000 \n",
    "word_vector_dim = 100  # Word2Vec 임베딩을 위해 100으로 설정 (랜덤 실험 시에는 16)\n",
    "max_length = 41\n",
    "hidden_dim = 128  # LSTM의 은닉 상태 차원\n",
    "num_layers = 2   # LSTM 레이어 수\n",
    "dropout = 0.5  # 드롭아웃 확률\n",
    "bidirectional = True # LSTM 양방향 설정\n",
    "filter_sizes = [2, 3, 4] # CNN 필터 크기\n",
    "\n",
    "# Word2Vec 임베딩 행렬을 사용한다고 가정하고, 이를 생성\n",
    "# (실제 Word2Vec 모델 로드 및 임베딩 행렬 생성 코드는 이전에 수행되었어야 함)\n",
    "# embedding_matrix = ...\n",
    "\n",
    "# 모델 인스턴스화\n",
    "model_lstm_cnn_word2vec = SentenceClassifier_LstmCnn(\n",
    "    embedding_matrix=embedding_matrix, \n",
    "    hidden_dim=hidden_dim, \n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout, \n",
    "    bidirectional=bidirectional, \n",
    "    filter_sizes=filter_sizes, \n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "# 1. 모델 임베딩 초기화 (Word2Vec 사용 시)\n",
    "# 모델의 임베딩 레이어를 사전 학습된 Word2Vec 벡터로 덮어씌운다.\n",
    "# 랜덤 임베딩을 사용하는 실험에서는 이 줄을 주석 처리해야 한다.\n",
    "# model_lstm_cnn_word2vec.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "\n",
    "# 2. 옵티마이저와 손실 함수 설정\n",
    "# Adam 최적화 도구와 이진 분류에 적합한 BCELoss 손실 함수를 사용한다.\n",
    "optimizer = optim.Adam(model_lstm_cnn_word2vec.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# 3. 데이터 로더 준비\n",
    "# 훈련 및 검증 데이터를 텐서로 변환하고 데이터셋, 데이터로더를 생성한다.\n",
    "partial_X_train_tensor = partial_X_train.detach().clone().to(torch.long)\n",
    "partial_y_train_tensor = torch.tensor(partial_y_train, dtype=torch.float)\n",
    "\n",
    "X_val_tensor = X_val.detach().clone().to(torch.long)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "train_dataset = TensorDataset(partial_X_train_tensor, partial_y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "epochs = 30  # 훈련 반복 횟수 설정\n",
    "train_losses = []  # 에포크별 훈련 손실을 저장할 리스트\n",
    "val_losses = []    # 에포크별 검증 손실을 저장할 리스트\n",
    "train_accs = []    # 에포크별 훈련 정확도를 저장할 리스트\n",
    "val_accs = []      # 에포크별 검증 정확도를 저장할 리스트\n",
    "\n",
    "# 4. 모델 훈련 및 검증 루프\n",
    "for epoch in range(epochs):\n",
    "    # 훈련 단계 시작\n",
    "    model_lstm_cnn_word2vec.train()  # 모델을 훈련 모드로 설정\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "        optimizer.zero_grad()  # 이전 스텝에서 계산된 기울기 초기화\n",
    "        outputs = model_lstm_cnn_word2vec(inputs)  # 모델에 입력 데이터를 넣어 예측값 계산\n",
    "        loss = loss_fn(outputs.squeeze(), labels)  # 예측값과 실제 라벨 간의 손실 계산\n",
    "        loss.backward()  # 손실에 대한 기울기 역전파\n",
    "        optimizer.step()  # 모델의 파라미터 업데이트\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()  # 예측값이 0.5보다 크면 긍정(1), 아니면 부정(0)으로 판단\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accs.append(correct / total)\n",
    "\n",
    "    # 검증 단계 시작\n",
    "    model_lstm_cnn_word2vec.eval()  # 모델을 평가 모드로 설정\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  # 기울기 계산 비활성화 (메모리 절약, 속도 향상)\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\"):\n",
    "            outputs = model_lstm_cnn_word2vec(inputs)\n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "            val_loss += loss.item()\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accs.append(val_correct / val_total)\n",
    "    \n",
    "    val_loss = val_losses[-1] # 현재 에포크의 검증 손실\n",
    "\n",
    "    # 손실 개선 여부 확인\n",
    "    if val_loss < best_loss - min_delta:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # 최적의 모델 가중치 저장\n",
    "        torch.save(model_lstm_cnn_word2vec.state_dict(), 'model_lstm_cnn_word2vec.pth')\n",
    "        print(f\"Validation loss improved. Saving model weights...\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Validation loss did not improve. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "    # 조기 종료 조건 확인\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        break\n",
    "    # 에포크별 결과 출력ㄴ\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"T_Loss: {train_losses[-1]:.4f}, T_Acc: {train_accs[-1]:.4f} - \"\n",
    "          f\"V_Loss: {val_losses[-1]:.4f}, V_Acc: {val_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81692543-6e03-4595-b448-69a3abb80fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim 형식으로 저장할 파일 경로\n",
    "word2vec_file_path = 'model_lstm_cnn_word2vec.txt'\n",
    "\n",
    "# 모델의 임베딩 가중치와 단어-인덱스 매핑 준비\n",
    "# 1. 모델 변수명을 'model_lstm_cnn_word2vec'로 수정\n",
    "vectors = model_lstm_cnn_word2vec.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "# 2. 모델 훈련 시 사용했던 실제 'word_to_index' 딕셔너리를 활용하여\n",
    "#    'index_to_word' 딕셔너리를 생성합니다.\n",
    "#    (이 부분은 실제 훈련 코드에 있는 'word_to_index' 변수명으로 교체해야 합니다.)\n",
    "#    예시: word_to_index = {'<pad>': 0, '<unk>': 1, '영화': 2, ...}\n",
    "#    - word_to_index 변수가 없으면 이 코드는 에러를 발생시킵니다.\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "\n",
    "# 파일에 쓰기\n",
    "with open(word2vec_file_path, 'w') as f:\n",
    "    # 1. 파일의 첫 줄에 '전체 단어 수', '벡터 차원'을 기록합니다.\n",
    "    #    gensim이 이 정보를 보고 파일 형식을 판단합니다.\n",
    "    vocab_size, word_vector_dim = vectors.shape\n",
    "    f.write(f'{vocab_size} {word_vector_dim}\\n')\n",
    "    \n",
    "    # 2. 각 단어의 벡터 값을 공백으로 구분하여 한 줄씩 기록합니다.\n",
    "    for i in range(vocab_size):\n",
    "        # 3. index_to_word.get(i, '<unk>')를 사용하여 없는 인덱스도 안전하게 처리\n",
    "        word = index_to_word.get(i, '<unk>')\n",
    "        vector_str = ' '.join(map(str, vectors[i, :]))\n",
    "        f.write(f'{word} {vector_str}\\n')\n",
    "\n",
    "print(f\"'{word2vec_file_path}' 파일에 임베딩 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33efeb0-0668-4cb8-8656-75e60cf5d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "model_lstm_cnn_word2vec.load_state_dict(torch.load('model_lstm_cnn_word2vec.pth'))\n",
    "model_lstm_cnn_word2vec.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model_lstm_cnn_word2vec(inputs)\n",
    "        loss = loss_fn(outputs.squeeze(), labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_correct / test_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4501b8-a815-4b6d-a87f-5bae3f05fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs_range, train_losses, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs_range, val_losses, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e4ec9-3289-4ae0-b39b-ae7836a57018",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs_range, train_accs, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs_range, val_accs, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f9e96-bcf1-488d-b32b-4a191b696066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23e1a5-10c4-4d45-a4a3-f5dbd6fb3111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6종 임배딩 비교\n",
    "#1. 코드 블록 제목: 6가지 모델 임베딩 파일 유사 단어 비교 (개선)\n",
    "# gensim과 OS 관련 라이브러리 임포트\n",
    "import os\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "# 2. 파일 경로 및 설정\n",
    "# 역할: 임베딩 파일들이 저장된 기본 경로를 설정\n",
    "# 목적: 모든 파일 경로를 간편하게 관리하기 위함입니다.\n",
    "base_path = os.path.join(os.getenv('HOME'), 'work', 'workplace', 'AIFFEL_quest_rs', 'Exploration', 'Quest05') + os.sep\n",
    "# 역할: 비교할 6가지 임베딩 파일 목록을 리스트로 정의\n",
    "# 목적: 반복문을 통해 각 파일을 순차적으로 처리하기 위함입니다.\n",
    "embedding_files = [\n",
    "    'model_lstm_random.txt',\n",
    "    'model_lstm_word2vec.txt',\n",
    "    'model_cnn_random.txt',\n",
    "    'model_cnn_word2vec.txt',\n",
    "    'model_lstm_cnn_random.txt',\n",
    "    'model_lstm_cnn_word2vec.txt'\n",
    "]\n",
    "# 역할: 유사 단어를 찾을 대상 단어 목록을 정의\n",
    "# 목적: 각 모델에서 동일한 단어들에 대한 유사 단어를 비교하기 위함입니다.\n",
    "target_words = ['재미', '취향', '추천', '영화', '배우', '음악']\n",
    "\n",
    "# 3. 임베딩 파일 로드\n",
    "# 역할: 로드된 gensim 모델들을 저장할 딕셔너리 초기화\n",
    "# 목적: 파일명(키)과 gensim 모델 객체(값)를 연결하여 편리하게 사용하기 위함입니다.\n",
    "word_vectors_models = {}\n",
    "print(\"--- 임베딩 파일 로드 시작 ---\")\n",
    "# 역할: embedding_files 리스트를 순회\n",
    "# 목적: 6개의 파일을 하나씩 로드하고 딕셔너리에 저장합니다.\n",
    "for filename in embedding_files:\n",
    "    file_path = base_path + filename\n",
    "    try:\n",
    "        # 역할: gensim load_word2vec_format 함수를 사용해 파일을 로드\n",
    "        # 목적: 텍스트 형식으로 저장된 임베딩 파일을 gensim 객체로 변환합니다.\n",
    "        word_vectors = Word2VecKeyedVectors.load_word2vec_format(file_path, binary=False)\n",
    "        word_vectors_models[filename] = word_vectors\n",
    "        print(f\"'{filename}' 로드 완료.\")\n",
    "    except FileNotFoundError:\n",
    "        # 역할: 파일이 존재하지 않을 경우 에러를 출력하고 다음 파일로 넘어감\n",
    "        # 목적: 파일 경로 오류로 인해 프로그램이 중단되는 것을 방지합니다.\n",
    "        print(f\"오류: '{filename}' 파일을 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "\n",
    "# 4. 유사 단어 비교 및 결과 출력\n",
    "print(\"\\n--- 유사 단어 비교 시작 ---\")\n",
    "# 역할: target_words 리스트를 순회\n",
    "# 목적: 각 단어에 대해 모든 모델에서 유사 단어를 찾습니다.\n",
    "for word in target_words:\n",
    "    print(f\"\\n==================== 단어: '{word}' ====================\")\n",
    "    # 역할: word_vectors_models 딕셔너리를 순회\n",
    "    # 목적: 모든 로드된 모델에 대해 유사 단어를 찾습니다.\n",
    "    for model_name, model in word_vectors_models.items():\n",
    "        # 역할: 현재 단어가 모델의 어휘 사전에 있는지 확인\n",
    "        # 목적: 사전에 없는 단어로 유사 단어를 찾으려 할 경우 에러가 나는 것을 방지합니다.\n",
    "        if word in model:\n",
    "            # 역할: gensim의 similar_by_word 함수를 사용해 유사 단어를 찾아 출력\n",
    "            # 목적: 각 모델이 생각하는 'word'와 가장 비슷한 단어들을 보여줍니다.\n",
    "            print(f\"[{model_name}] 유사 단어:\")\n",
    "            similar_words = model.similar_by_word(word, topn=5)\n",
    "            # 역할: 찾은 유사 단어를 보기 좋게 출력\n",
    "            # 목적: 각 결과가 어떤 단어인지, 유사도 점수는 얼마인지 명확하게 표시합니다.\n",
    "            for similar_word, score in similar_words:\n",
    "                print(f\"  - {similar_word} (유사도: {score:.4f})\")\n",
    "        else:\n",
    "            # 역할: 단어가 사전에 없을 경우 메시지 출력\n",
    "            # 목적: 해당 모델이 이 단어를 학습하지 못했음을 알려줍니다.\n",
    "            print(f\"[{model_name}] 사전에 '{word}' 단어가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373ac15-9d33-4de9-9fbb-b35960a5c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 라이브러리 및 모델 클래스 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 모델 클래스 정의\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, word_vector_dim):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, word_vector_dim)\n",
    "        self.lstm = nn.LSTM(word_vector_dim, 8, batch_first=True)\n",
    "        self.fc1 = nn.Linear(8, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class SentenceClassifier_Cnn(nn.Module):\n",
    "    def __init__(self, embedding_matrix, filter_sizes, max_length, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=embedding_matrix.shape[0],\n",
    "            embedding_dim=embedding_matrix.shape[1]\n",
    "        )\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        embedding_dim = self.embedding.weight.shape[1]\n",
    "\n",
    "        conv = []\n",
    "        for size in filter_sizes:\n",
    "            conv.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=embedding_dim,\n",
    "                        out_channels=1,\n",
    "                        kernel_size=size\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool1d(kernel_size=max_length-size-1),\n",
    "                )\n",
    "            )\n",
    "        self.conv_filters = nn.ModuleList(conv)\n",
    "        output_size = len(filter_sizes)\n",
    "        self.pre_classifier = nn.Linear(output_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(output_size, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        conv_outputs = [conv(embeddings) for conv in self.conv_filters]\n",
    "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
    "        logits = self.pre_classifier(concat_outputs)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.classifier(logits)\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "class SentenceClassifier_LstmCnn(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, num_layers, dropout, bidirectional, filter_sizes, max_length):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=embedding_matrix.shape[0],\n",
    "            embedding_dim=embedding_matrix.shape[1]\n",
    "        )\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        embedding_dim = self.embedding.weight.shape[1]\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        conv = []\n",
    "        for size in filter_sizes:\n",
    "            conv.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=lstm_output_dim,\n",
    "                        out_channels=1,\n",
    "                        kernel_size=size\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool1d(kernel_size=max_length-size-1),\n",
    "                )\n",
    "            )\n",
    "        self.conv_filters = nn.ModuleList(conv)\n",
    "        output_size = len(filter_sizes)\n",
    "        self.pre_classifier = nn.Linear(output_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(output_size, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        lstm_output, _ = self.lstm(embeddings)\n",
    "        lstm_output = lstm_output.permute(0, 2, 1)\n",
    "        conv_outputs = [conv(lstm_output) for conv in self.conv_filters]\n",
    "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
    "        logits = self.pre_classifier(concat_outputs)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.classifier(logits)\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "\n",
    "# 1. 모델 파일 경로 및 설정 정보\n",
    "base_path = os.path.join(os.getenv('HOME'), 'work', 'workplace', 'AIFFEL_quest_rs', 'Exploration', 'Quest05') + os.sep\n",
    "model_configs = {\n",
    "    'lstm_random': {'path': base_path + 'model_lstm_random.pth', 'class': SentimentModel, 'params': {'vocab_size': 10000, 'word_vector_dim': 16}},\n",
    "    'lstm_word2vec': {'path': base_path + 'model_lstm_word2vec.pth', 'class': SentimentModel, 'params': {'vocab_size': 10000, 'word_vector_dim': 100}},\n",
    "    'cnn_random': {'path': base_path + 'model_cnn_random.pth', 'class': SentenceClassifier_Cnn, 'params': {'embedding_matrix': np.random.rand(10000, 16), 'filter_sizes': [2, 3, 4], 'max_length': 41, 'dropout': 0.5}},\n",
    "    'cnn_word2vec': {'path': base_path + 'model_cnn_word2vec.pth', 'class': SentenceClassifier_Cnn, 'params': {'embedding_matrix': np.random.rand(10000, 100), 'filter_sizes': [2, 3, 4], 'max_length': 41, 'dropout': 0.5}},\n",
    "    'lstm_cnn_random': {'path': base_path + 'model_lstm_cnn_random.pth', 'class': SentenceClassifier_LstmCnn, 'params': {'embedding_matrix': np.random.rand(10000, 16), 'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.5, 'bidirectional': True, 'filter_sizes': [2, 3, 4], 'max_length': 41}},\n",
    "    'lstm_cnn_word2vec': {'path': base_path + 'model_lstm_cnn_word2vec.pth', 'class': SentenceClassifier_LstmCnn, 'params': {'embedding_matrix': np.random.rand(10000, 100), 'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.5, 'bidirectional': True, 'filter_sizes': [2, 3, 4], 'max_length': 41}}\n",
    "}\n",
    "\n",
    "embedding_weights = {}\n",
    "\n",
    "# 2. 각 모델 로드 및 임베딩 가중치 추출\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"--- {model_name} 임베딩 가중치 추출 중 ---\")\n",
    "    \n",
    "    try:\n",
    "        # 모델 인스턴스화\n",
    "        model = config['class'](**config['params'])\n",
    "        \n",
    "        # 저장된 가중치 로드\n",
    "        model.load_state_dict(torch.load(config['path']))\n",
    "        \n",
    "        # 임베딩 가중치 추출 및 저장\n",
    "        # .detach().cpu().numpy()를 사용하여 텐서를 넘파이 배열로 변환\n",
    "        embedding_weights[model_name] = model.embedding.weight.data.detach().cpu().numpy()\n",
    "        \n",
    "        print(f\"{model_name} 임베딩 가중치 추출 완료. Shape: {embedding_weights[model_name].shape}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: {config['path']} 파일을 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "    except Exception as e:\n",
    "        print(f\"오류: {model_name} 모델 로드 및 가중치 추출 실패 - {e}\")\n",
    "\n",
    "# 3. 임베딩 가중치 간의 유사도(코사인 유사도) 계산 및 비교\n",
    "print(\"\\n### 임베딩 가중치 간의 코사인 유사도 비교 ###\")\n",
    "\n",
    "# 모든 모델 쌍에 대한 유사도를 저장할 딕셔너리\n",
    "similarity_matrix = np.zeros((len(embedding_weights), len(embedding_weights)))\n",
    "model_names = list(embedding_weights.keys())\n",
    "\n",
    "# 유사도 계산\n",
    "for i, name1 in enumerate(model_names):\n",
    "    for j, name2 in enumerate(model_names):\n",
    "        # 크기가 다른 임베딩을 비교하기 위해 크기가 더 작은 임베딩에 맞춰 슬라이싱\n",
    "        dim1 = embedding_weights[name1].shape[1]\n",
    "        dim2 = embedding_weights[name2].shape[1]\n",
    "        min_dim = min(dim1, dim2)\n",
    "        \n",
    "        vec1 = embedding_weights[name1][:, :min_dim]\n",
    "        vec2 = embedding_weights[name2][:, :min_dim]\n",
    "\n",
    "        # 코사인 유사도 계산\n",
    "        # 1. 벡터를 단위 벡터로 정규화\n",
    "        vec1_norm = vec1 / np.linalg.norm(vec1, axis=1, keepdims=True)\n",
    "        vec2_norm = vec2 / np.linalg.norm(vec2, axis=1, keepdims=True)\n",
    "        \n",
    "        # 2. 내적 계산 (코사인 유사도)\n",
    "        # 내적의 평균을 유사도로 사용\n",
    "        similarity = np.mean(np.sum(vec1_norm * vec2_norm, axis=1))\n",
    "        similarity_matrix[i, j] = similarity\n",
    "\n",
    "# 4. 결과 시각화\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=model_names, columns=model_names)\n",
    "print(similarity_df)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_df, annot=True, cmap='viridis', fmt=\".4f\")\n",
    "plt.title('Embedding Weight Cosine Similarity Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 5. 상세 분석\n",
    "print(\"\\n### 분석 결과 ###\")\n",
    "print(\"- 각 행과 열은 모델의 임베딩 가중치를 나타냅니다.\")\n",
    "print(\"- 히트맵의 값은 두 임베딩 가중치 벡터 간의 평균 코사인 유사도를 의미합니다.\")\n",
    "print(\"- 값이 1에 가까울수록 두 임베딩 가중치가 매우 유사하다는 뜻입니다.\")\n",
    "print(\"- Word2Vec 모델들 간의 유사도, Random 모델들 간의 유사도를 비교해보세요.\")\n",
    "print(\"- 예를 들어, 'lstm_word2vec'과 'cnn_word2vec'의 유사도는 'lstm_random'과 'cnn_random'의 유사도와 어떻게 다른가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637f658-09bd-4bfc-9e77-19545fe91f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdef661-8d41-42cc-a3ea-abb5f273a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 자체 학습한 임베딩을 `gensim` 형식으로 저장 및 로드하는 코드\n",
    "#    (이전에 수행했던 코드)\n",
    "import os\n",
    "import torch\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models import Word2Vec # Word2Vec 모델 로드를 위해 추가\n",
    "import numpy as np\n",
    "\n",
    "# model, vocab_size, word_vector_dim, index_to_word 변수가 정의되었다고 가정\n",
    "word2vec_file_path = os.path.join(os.getenv('HOME'), 'work', 'sentiment_classification', 'word2vec_emb', 'word2vec.txt')\n",
    "with open(word2vec_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('{} {}\\n'.format(vocab_size - 4, word_vector_dim))\n",
    "    vectors = model.embedding.weight.detach().cpu().numpy()\n",
    "    for i in range(4, vocab_size):\n",
    "        f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, vectors[i, :]))))\n",
    "\n",
    "# 자체 학습 임베딩 로드\n",
    "word_vectors_self = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "\n",
    "# 1. 사전 학습된 한국어 Word2Vec 임베딩 로드\n",
    "#    (프로젝트 요구사항에 따라 word2vec_ko.model 사용)\n",
    "korean_word2vec_path = os.path.join(os.getenv('HOME'), 'work', 'sentiment_classification', 'data', 'word2vec_ko.model')\n",
    "word_vectors_ko = Word2Vec.load(korean_word2vec_path).wv\n",
    "\n",
    "# 2. 특정 단어에 대한 유사 단어 비교 분석\n",
    "def compare_similar_words(word, topn=10):\n",
    "    print(f\"================== '{word}' 유사 단어 비교 ==================\")\n",
    "    \n",
    "    # 자체 학습 임베딩 결과\n",
    "    # word_vectors_self의 key_to_index는 단어를 기준으로 확인\n",
    "    if word in word_vectors_self.key_to_index:\n",
    "        print(\"\\n[자체 학습 임베딩 결과]\")\n",
    "        # gensim에서 유사 단어 찾기\n",
    "        for similar_word, score in word_vectors_self.similar_by_word(word, topn=topn):\n",
    "            print(f\"{similar_word}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n[자체 학습 임베딩] '{word}' 단어가 어휘 사전에 없습니다.\")\n",
    "\n",
    "    # 사전 학습 임베딩 결과\n",
    "    # word_vectors_ko의 key_to_index는 단어를 기준으로 확인\n",
    "    if word in word_vectors_ko.key_to_index:\n",
    "        print(\"\\n[사전 학습 임베딩 (word2vec_ko.model) 결과]\")\n",
    "        # gensim에서 유사 단어 찾기\n",
    "        for similar_word, score in word_vectors_ko.similar_by_word(word, topn=topn):\n",
    "            print(f\"{similar_word}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n[사전 학습 임베딩] '{word}' 단어가 어휘 사전에 없습니다.\")\n",
    "\n",
    "# 3. 비교 실행 (한국어 단어 사용)\n",
    "compare_similar_words(\"재미\", topn=5)\n",
    "compare_similar_words(\"취향\", topn=5)\n",
    "compare_similar_words(\"추천\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cbf964-f70d-41f3-8bf6-ca8a20ab877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 라이브러리 및 모델 클래스 임포트\n",
    "# 필요한 라이브러리를 임포트합니다.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, Any\n",
    "\n",
    "# 모델 클래스 정의\n",
    "# 1_1과 1_2 실험에서 사용한 LSTM 모델 클래스입니다.\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, word_vector_dim):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, word_vector_dim)\n",
    "        self.lstm = nn.LSTM(word_vector_dim, 8, batch_first=True)\n",
    "        self.fc1 = nn.Linear(8, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# 2_1과 2_2 실험에서 사용한 CNN 모델 클래스입니다.\n",
    "class SentenceClassifier_Cnn(nn.Module):\n",
    "    def __init__(self, embedding_matrix, filter_sizes, max_length, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=embedding_matrix.shape[0],\n",
    "            embedding_dim=embedding_matrix.shape[1]\n",
    "        )\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        embedding_dim = self.embedding.weight.shape[1]\n",
    "\n",
    "        conv = []\n",
    "        for size in filter_sizes:\n",
    "            # Conv1d의 out_channels를 128로 설정하여 더 많은 특징을 추출하도록 수정\n",
    "            conv.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=embedding_dim,\n",
    "                        out_channels=128, \n",
    "                        kernel_size=size\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    # MaxPool1d를 AdaptiveMaxPool1d(1)로 변경하여 유연하게 풀링하도록 수정\n",
    "                    nn.AdaptiveMaxPool1d(1),\n",
    "                )\n",
    "            )\n",
    "        self.conv_filters = nn.ModuleList(conv)\n",
    "        # out_channels=128로 변경했으므로, output_size도 (필터 개수 * 필터 수)로 변경\n",
    "        output_size = len(filter_sizes) * 128\n",
    "        self.pre_classifier = nn.Linear(output_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(output_size, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        conv_outputs = [conv(embeddings) for conv in self.conv_filters]\n",
    "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
    "        logits = self.pre_classifier(concat_outputs)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.classifier(logits)\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "# 3_1과 3_2 실험에서 사용한 LSTM-CNN 모델 클래스입니다.\n",
    "class SentenceClassifier_LstmCnn(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, num_layers, dropout, bidirectional, filter_sizes, max_length):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=embedding_matrix.shape[0],\n",
    "            embedding_dim=embedding_matrix.shape[1]\n",
    "        )\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        embedding_dim = self.embedding.weight.shape[1]\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        conv = []\n",
    "        for size in filter_sizes:\n",
    "            # Conv1d의 out_channels를 128로 설정하여 더 많은 특징을 추출하도록 수정\n",
    "            conv.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=lstm_output_dim,\n",
    "                        out_channels=128,\n",
    "                        kernel_size=size\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    # MaxPool1d를 AdaptiveMaxPool1d(1)로 변경하여 유연하게 풀링하도록 수정\n",
    "                    nn.AdaptiveMaxPool1d(1),\n",
    "                )\n",
    "            )\n",
    "        self.conv_filters = nn.ModuleList(conv)\n",
    "        # out_channels=128로 변경했으므로, output_size도 (필터 개수 * 필터 수)로 변경\n",
    "        output_size = len(filter_sizes) * 128\n",
    "        self.pre_classifier = nn.Linear(output_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(output_size, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        lstm_output, _ = self.lstm(embeddings)\n",
    "        lstm_output = lstm_output.permute(0, 2, 1)\n",
    "        conv_outputs = [conv(lstm_output) for conv in self.conv_filters]\n",
    "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
    "        logits = self.pre_classifier(concat_outputs)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.classifier(logits)\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "# 1. 모델 파일 경로 및 설정 정보\n",
    "# 이전에 훈련하여 저장한 6개의 모델 파일 경로와 설정을 정의합니다.\n",
    "base_path = os.path.join(os.getenv('HOME'), 'work', 'workplace', 'AIFFEL_quest_rs', 'Exploration', 'Quest05') + os.sep\n",
    "model_configs = OrderedDict([\n",
    "    ('lstm_random', {'path': base_path + 'model_lstm_random.pth', 'class': SentimentModel, 'params': {'vocab_size': 10000, 'word_vector_dim': 16}}),\n",
    "    ('lstm_word2vec', {'path': base_path + 'model_lstm_word2vec.pth', 'class': SentimentModel, 'params': {'vocab_size': 10000, 'word_vector_dim': 100}}),\n",
    "    ('cnn_random', {'path': base_path + 'model_cnn_random.pth', 'class': SentenceClassifier_Cnn, 'params': {'embedding_matrix': np.random.rand(10000, 16), 'filter_sizes': [2, 3, 4], 'max_length': 41, 'dropout': 0.5}}),\n",
    "    ('cnn_word2vec', {'path': base_path + 'model_cnn_word2vec.pth', 'class': SentenceClassifier_Cnn, 'params': {'embedding_matrix': np.random.rand(10000, 100), 'filter_sizes': [2, 3, 4], 'max_length': 41, 'dropout': 0.5}}),\n",
    "    ('lstm_cnn_random', {'path': base_path + 'model_lstm_cnn_random.pth', 'class': SentenceClassifier_LstmCnn, 'params': {'embedding_matrix': np.random.rand(10000, 16), 'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.5, 'bidirectional': True, 'filter_sizes': [2, 3, 4], 'max_length': 41}}),\n",
    "    ('lstm_cnn_word2vec', {'path': base_path + 'model_lstm_cnn_word2vec.pth', 'class': SentenceClassifier_LstmCnn, 'params': {'embedding_matrix': np.random.rand(10000, 100), 'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.5, 'bidirectional': True, 'filter_sizes': [2, 3, 4], 'max_length': 41}}),\n",
    "])\n",
    "\n",
    "# 2. 단어-인덱스 매핑 정보 생성 (더미 데이터)\n",
    "# 실제 프로젝트에서는 이전 단계에서 저장된 `index_to_word`를 사용해야 합니다.\n",
    "# 여기서는 예시를 위해 간단하게 생성합니다.\n",
    "index_to_word_path = os.path.join(os.getenv('HOME'), 'work', 'workplace', 'AIFFEL_quest_rs', 'Exploration', 'Quest05', 'index_to_word.json')\n",
    "try:\n",
    "    import json\n",
    "    with open(index_to_word_path, 'r', encoding='utf-8') as f:\n",
    "        index_to_word = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"경고: {index_to_word_path} 파일을 찾을 수 없습니다. 더미 데이터를 생성합니다.\")\n",
    "    index_to_word = {i: f\"단어{i}\" for i in range(10000)}\n",
    "    index_to_word[0] = '<pad>'\n",
    "    index_to_word[1] = '<unk>'\n",
    "    index_to_word[2] = '재미'\n",
    "    index_to_word[3] = '취향'\n",
    "    index_to_word[4] = '추천'\n",
    "    index_to_word[5] = '영화'\n",
    "    index_to_word[6] = '배우'\n",
    "\n",
    "# gensim 형식으로 변환된 임베딩을 저장할 딕셔너리\n",
    "gensim_embeddings = {}\n",
    "\n",
    "# 3. 각 모델 로드 및 임베딩 가중치 추출 후 gensim 형식으로 변환\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"--- '{model_name}' 임베딩 가중치 추출 및 gensim 변환 중 ---\")\n",
    "    \n",
    "    try:\n",
    "        # 모델 인스턴스화\n",
    "        model = config['class'](**config['params'])\n",
    "        \n",
    "        # 저장된 가중치 로드\n",
    "        model.load_state_dict(torch.load(config['path']))\n",
    "        \n",
    "        # 임베딩 가중치 추출\n",
    "        embedding_matrix = model.embedding.weight.data.detach().cpu().numpy()\n",
    "        \n",
    "        # gensim KeyedVectors 객체 생성\n",
    "        word_vectors = Word2VecKeyedVectors(vector_size=embedding_matrix.shape[1])\n",
    "        word_vectors.add_vectors(\n",
    "            list(index_to_word.values()), # 단어 리스트\n",
    "            embedding_matrix            # 임베딩 행렬\n",
    "        )\n",
    "        gensim_embeddings[model_name] = word_vectors\n",
    "        \n",
    "        print(f\"'{model_name}' 임베딩 변환 완료. 벡터 크기: {embedding_matrix.shape[1]}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: {config['path']} 파일을 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "    except Exception as e:\n",
    "        print(f\"오류: '{model_name}' 모델 로드 및 변환 실패 - {e}\")\n",
    "\n",
    "# 4. 특정 단어에 대한 유사 단어 비교 분석 함수\n",
    "def compare_similar_words_for_all_models(word: str, topn: int = 5):\n",
    "    \"\"\"\n",
    "    6가지 모델의 임베딩을 사용하여 특정 단어와 유사한 단어를 비교합니다.\n",
    "    \"\"\"\n",
    "    print(f\"\\n================== '{word}' 유사 단어 비교 ==================\")\n",
    "    for model_name, word_vectors in gensim_embeddings.items():\n",
    "        print(f\"\\n--- {model_name} 결과 (차원: {word_vectors.vector_size}) ---\")\n",
    "        if word in word_vectors.key_to_index:\n",
    "            try:\n",
    "                # gensim에서 유사 단어 찾기\n",
    "                for similar_word, score in word_vectors.similar_by_word(word, topn=topn):\n",
    "                    print(f\"  - {similar_word}: {score:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - 유사 단어 찾기 실패: {e}\")\n",
    "        else:\n",
    "            print(f\"  - '{word}' 단어가 어휘 사전에 없습니다.\")\n",
    "\n",
    "# 5. 비교 실행 (한국어 단어 사용)\n",
    "# 6가지 모델에 대한 유사 단어 비교를 수행합니다.\n",
    "compare_similar_words_for_all_models(\"재미\", topn=5)\n",
    "compare_similar_words_for_all_models(\"취향\", topn=5)\n",
    "compare_similar_words_for_all_models(\"추천\", topn=5)\n",
    "compare_similar_words_for_all_models(\"영화\", topn=5)\n",
    "compare_similar_words_for_all_models(\"배우\", topn=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a6743-d984-4835-8a27-1fed2606df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 코드의 목적: PyTorch 모델 파일에서 임베딩 벡터를 추출하여 gensim 방식으로 확인\n",
    "\n",
    "# 1. 필요한 라이브러리 임포트\n",
    "# PyTorch 모델을 다루기 위한 라이브러리\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# 넘파이 배열로 변환하기 위한 라이브러리\n",
    "import numpy as np\n",
    "# gensim KeyedVectors 객체를 생성하기 위한 라이브러리\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "# 파일 경로 설정을 위한 라이브러리\n",
    "import os\n",
    "\n",
    "# 2. PyTorch 모델 클래스 정의 (lstm_random 모델용)\n",
    "class SentimentModel(nn.Module):\n",
    "    # 모델의 생성자 함수입니다.\n",
    "    def __init__(self, vocab_size, word_vector_dim):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        # nn.Embedding 레이어를 정의합니다. 이 레이어에 단어 임베딩이 저장됩니다.\n",
    "        self.embedding = nn.Embedding(vocab_size, word_vector_dim)\n",
    "        # LSTM, Linear 등 다른 레이어들도 정의하지만, 여기서는 임베딩만 사용합니다.\n",
    "        self.lstm = nn.LSTM(word_vector_dim, 8, batch_first=True)\n",
    "        self.fc1 = nn.Linear(8, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "\n",
    "    # 모델의 순전파(forward) 함수입니다.\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# 3. 모델 파일 경로 및 설정 정의\n",
    "# 사용자의 환경에 맞게 경로를 설정합니다.\n",
    "base_path = os.path.join(os.getenv('HOME'), 'work', 'workplace', 'AIFFEL_quest_rs', 'Exploration', 'Quest05') + os.sep\n",
    "model_file_path = base_path + 'model_lstm_random.pth'\n",
    "vocab_size = 10000\n",
    "word_vector_dim = 16\n",
    "\n",
    "# 4. 모델 로딩 및 임베딩 가중치 추출\n",
    "# 모델 클래스의 인스턴스를 생성합니다.\n",
    "model = SentimentModel(vocab_size, word_vector_dim)\n",
    "# 저장된 가중치를 모델에 로드합니다.\n",
    "model.load_state_dict(torch.load(model_file_path))\n",
    "# 모델의 임베딩 레이어에서 가중치(weight)를 추출하고, 넘파이 배열로 변환합니다.\n",
    "embedding_matrix = model.embedding.weight.data.detach().cpu().numpy()\n",
    "\n",
    "# 5. 간단한 단어-인덱스 매핑 정의\n",
    "# 실제 단어를 인덱스와 매핑하는 딕셔너리를 간단히 정의합니다.\n",
    "index_to_word = {\n",
    "    0: '<pad>', 1: '<unk>', 2: '재미', 3: '취향', 4: '추천',\n",
    "    5: '영화', 6: '배우', 7: '음악', 8: '친구'\n",
    "}\n",
    "\n",
    "# 6. gensim KeyedVectors 객체 생성\n",
    "# 추출한 가중치와 단어 매핑 정보를 이용하여 KeyedVectors 객체를 만듭니다.\n",
    "word_vectors = Word2VecKeyedVectors(vector_size=word_vector_dim)\n",
    "word_vectors.add_vectors(\n",
    "    list(index_to_word.values()),   # 단어 리스트\n",
    "    embedding_matrix[:len(index_to_word)]  # 해당 단어에 맞는 임베딩 벡터만 사용\n",
    ")\n",
    "\n",
    "# 7. 원하는 단어의 벡터 확인\n",
    "# '재미'라는 단어의 임베딩 벡터를 출력합니다.\n",
    "print(\"================ '재미' 단어의 임베딩 벡터 확인 ================\")\n",
    "if '재미' in word_vectors:\n",
    "    # KeyedVectors 객체에서 '재미' 단어의 벡터를 가져옵니다.\n",
    "    vector = word_vectors['재미']\n",
    "    # 벡터의 처음 5개 요소를 출력하여 정상적으로 로드되었음을 확인합니다.\n",
    "    print(f\"['재미'] 단어의 벡터 (일부): {vector[:5]}\")\n",
    "    # '재미'와 가장 유사한 단어들을 출력합니다.\n",
    "    print(f\"\\n['재미']와 유사한 단어: {word_vectors.similar_by_word('재미', topn=3)}\")\n",
    "else:\n",
    "    print(\"'재미' 단어가 어휘 사전에 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f0620-09fb-4833-a053-4a52b0d1201c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
